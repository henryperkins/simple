{"350f659af6f0e50f1b58d5172c298c38cca048bcd5def0871ad70a09e95efd50": {"value": {"summary": "Manages code chunking operations for various programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens allowed to overlap between consecutive chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the `ChunkManager` class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any general errors during the chunking process."}], "description": "The `ChunkManager` class is designed to handle the creation, splitting, and merging of code chunks. It supports context-aware chunking for Python code using Abstract Syntax Tree (AST) analysis and simple line-based chunking for other languages. The class ensures that code chunks do not exceed a specified token limit, allowing for overlap between chunks to maintain context. It utilizes a `TokenManager` to count tokens and manage chunk sizes effectively.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733741408.159387}, "80976e55ccd95f6e8ed3455b8489b494a4c98aecd4424f1a0622c5448fb5c75a": {"value": {"summary": "Analyzes dependencies within a Python file using Abstract Syntax Tree (AST).", "args": [{"name": "code", "type": "str", "description": "The source code to be analyzed for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of strings representing the dependencies found in the provided code."}, "raises": [{"exception": "SyntaxError", "description": "Raised when the provided code has syntax errors that prevent it from being parsed."}], "description": "The DependencyAnalyzer class is designed to analyze Python source code to identify dependencies using the Abstract Syntax Tree (AST) module. It traverses the AST of the provided code to find import statements and function calls, collecting all the dependencies that are explicitly imported and used within the code. This analysis helps in understanding the external modules and functions that the code relies on, which is useful for code auditing, refactoring, and dependency management.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741410.3258276}, "bd8e7c3d97003f84179ece92d480746d301a52fc312e864a9f3909542c45000c": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between consecutive chunks."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any other errors encountered during the chunking process."}], "description": "The `ChunkManager` class is responsible for managing code chunking operations, particularly for Python code. It provides methods to create, split, and merge code chunks, ensuring that each chunk adheres to a specified maximum token limit. The class uses Abstract Syntax Tree (AST) analysis for Python code to create context-aware chunks and a simple line-based approach for other languages. The class also handles token counting and manages overlaps between chunks to maintain context.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733741568.6243107}, "62566fd2d36d1f80703a158c805378b15fa6392a95e1c920faaff1787f95f3f6": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to be analyzed for dependencies."}], "returns": {"type": "Set[str]", "description": "A set containing the names of dependencies found in the provided code."}, "raises": [{"exception": "SyntaxError", "description": "Raised when the provided code contains syntax errors that prevent parsing."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code to identify dependencies by utilizing the Abstract Syntax Tree (AST) module. It traverses the AST of the provided code to detect import statements and function calls, recording the modules and functions that are used. This is useful for understanding the external dependencies of a Python script, which can aid in refactoring, dependency management, and understanding code structure.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741571.2905579}, "11db222e3378d0073cfd39c6887f92e59652fafadd5feb2ac5bd848c2e201ece": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk. Default is 4096."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between chunks to preserve context. Default is 200."}], "returns": {"type": "ChunkManager", "description": "An instance of the `ChunkManager` class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any general errors encountered during chunk creation."}], "description": "The `ChunkManager` class is responsible for managing the operations related to code chunking. It provides functionalities to create, split, and merge code chunks from source code files. The class utilizes a `TokenManager` to handle token counting and ensures that the chunks created do not exceed a specified maximum token limit, with an optional overlap for context preservation. It supports context-aware chunking for Python code using AST analysis and simple line-based chunking for other languages.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733741581.6992395}, "cbb8d6df2b3a9b65ced339590d4fc26681242ea1f6a4cee008a6f91627f254e2": {"value": {"summary": "Manages code chunking operations for various programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens that can overlap between chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class."}, "raises": [{"exception": "SyntaxError", "description": "Raised if the code contains syntax errors during AST parsing."}, {"exception": "Exception", "description": "Raised for any other errors encountered during chunk creation."}], "description": "The `ChunkManager` class provides functionality to manage and manipulate code chunks, particularly for Python code. It supports creating chunks with context awareness using Abstract Syntax Tree (AST) analysis for Python, as well as simpler chunking methods for other languages. It also provides methods to split and merge chunks, ensuring that the operations respect token limits and overlaps. The class utilizes a `TokenManager` to count tokens and manage chunk sizes appropriately.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733741656.4757566}, "d45f7eb59093084ed5b43cca87adead963498553846da725c78edd5d8624c50a": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of strings representing the dependencies found in the given code."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the provided source code during parsing."}], "description": "The `DependencyAnalyzer` class is designed to analyze dependencies within a Python source code file by utilizing the Abstract Syntax Tree (AST) module. It traverses the AST to identify and record all imported modules and functions that are used within the code. This is particularly useful for understanding the dependencies of a script or module, which can aid in tasks such as dependency management, code refactoring, or static analysis. The class extends `ast.NodeVisitor` to leverage the visitor pattern for traversing the AST nodes.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741659.546918}, "a8bd634606198e4ca01511d848d16a1ec2316a1b0a21debeb61802ad6445404e": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "Flag indicating whether to use the cache for reading the file."}, {"name": "encoding", "type": "str", "description": "The encoding to be used for reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised when the file cannot be decoded with the specified encoding."}, {"exception": "Exception", "description": "Raised for any other errors that occur during file reading."}], "description": "The `FileHandler` class provides methods to perform asynchronous file operations with support for caching and error handling. It is designed to efficiently read file contents while minimizing disk I/O by utilizing a caching mechanism. The class can handle various exceptions that may occur during file operations, such as `UnicodeDecodeError`, and provides fallback mechanisms to ensure robust file reading. The class uses a thread-safe cache to store file contents and manages cache size to prevent excessive memory usage.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741665.4244902}, "5ce2060988e394ba52aa8621fd4f860094ff2a9c79df0e88b76f0c731820a38f": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This exception does not return a value."}, "raises": [], "description": "The `MetricsCalculationError` class serves as a custom exception for handling errors related to metrics calculations. It provides a specific exception type to distinguish metrics calculation errors from other types of exceptions.", "metadata": {}, "complexity": 0, "validation_status": true, "validation_errors": []}, "timestamp": 1733741667.1118214}, "c734c9dd9e6c8af8c0ebf5ac334bb6e878fffe2b719ccbaa6fe7fe494625cb5e": {"value": {"summary": "Exception raised when a requested chunk cannot be found.", "args": [], "returns": {"type": "None", "description": "This exception does not return a value as it is used for error signaling."}, "raises": [], "description": "The `ChunkNotFoundError` is a custom exception that is raised within the context management system when an operation attempts to access a code chunk that does not exist in the current context tree. This exception helps in identifying and handling cases where a requested chunk is missing, providing a clear indication of the issue to the caller.", "metadata": {}, "complexity": 0, "validation_status": true, "validation_errors": []}, "timestamp": 1733741669.6774063}, "ee61cf8644749b85546b1473dd33f857188ab6daa9952423a52a1e20297dcbff": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between consecutive chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the code being chunked."}, {"exception": "Exception", "description": "Raised for any other errors that occur during chunk creation."}], "description": "The `ChunkManager` class is responsible for managing the operations related to code chunking. It provides methods to create, split, and merge code chunks, specifically designed to handle large code files by breaking them into manageable pieces. The class supports context-aware chunking for Python code using Abstract Syntax Tree (AST) analysis and simple line-based chunking for other languages. It utilizes a `TokenManager` to ensure that chunks do not exceed a specified token limit, allowing for overlap between chunks to maintain context. The class handles exceptions gracefully, logging errors when chunk creation fails due to syntax errors or other issues.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733741763.219836}, "9d24b0bbdf04db214a7f3239baf551a488115763ec8a27f02ca0d5382a241f23": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to be analyzed for dependencies."}], "returns": {"type": "Set[str]", "description": "A set containing the names of dependencies found in the code."}, "raises": [{"exception": "SyntaxError", "description": "Raised if the provided code contains syntax errors that prevent parsing."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code to identify dependencies by traversing the Abstract Syntax Tree (AST) of the code. It extends the `ast.NodeVisitor` class to visit specific nodes related to imports and function calls, determining which modules and functions are used within the code. This is useful for understanding the external dependencies of a Python script, which can aid in refactoring, dependency management, and code analysis tasks.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741766.7493303}, "e7f4812d15953b23994dbb2576b256a9a865607f436c45f919b7942bc0851d85": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "A flag indicating whether to use cached content if available."}, {"name": "encoding", "type": "str", "description": "The encoding to use for reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised when there is an encoding issue with the file."}, {"exception": "Exception", "description": "Raised for any other errors that occur during file reading."}], "description": "The `FileHandler` class provides methods to perform asynchronous file operations with support for caching and error handling. It is designed to efficiently read file contents while minimizing repeated disk access by caching file contents in memory. The class uses a thread-safe cache mechanism to store file contents and supports a maximum cache size to prevent excessive memory usage. Additionally, it handles common file reading errors such as encoding issues and logs appropriate warnings or errors.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741770.342872}, "f4626b7a1ead15aa7edc087a83f9940a6639a456610b7e11bce184f27c80d593": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [], "description": "The `MetricsCalculationError` class serves as a custom exception for handling errors specifically related to metrics calculations in the module. It inherits from the built-in `Exception` class, allowing it to be used in try-except blocks to catch and handle errors that occur during metrics calculations.", "metadata": {}, "complexity": 0, "validation_status": true, "validation_errors": []}, "timestamp": 1733741772.5841742}, "7ef6f7150aba9c1fd1ed91bc56df9fe7748270108fa744ec2ddb3ae7587b8dd2": {"value": {"summary": "Exception raised when a requested chunk cannot be found.", "args": [], "returns": {"type": "None", "description": "This exception does not return any value."}, "raises": [], "description": "The ChunkNotFoundError is an exception that is raised when a requested code chunk cannot be located within the context manager. This exception is used to signal that an operation requiring a specific chunk cannot proceed because the chunk is missing.", "metadata": {}, "complexity": 0, "validation_status": true, "validation_errors": []}, "timestamp": 1733741776.9228811}, "3f5dbba00ae45881a9fd576d2619ffca2e94a98fa7d7cd3555f6bb8b7ee469cc": {"value": {"summary": "Represents an interface to interact with OpenAI's GPT-4o model.", "args": [{"name": "api_key", "type": "str", "description": "The API key used to authenticate requests to OpenAI's API."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class."}, "raises": [{"exception": "openai.error.OpenAIError", "description": "If there is an error communicating with the OpenAI API."}], "description": "The OpenAIModel class provides methods to create prompts, calculate token usage, and generate documentation using OpenAI's GPT-4o model. It handles the integration with OpenAI's API by setting the API key and using it to make requests. The class is designed to facilitate the generation of structured prompts and to manage token calculations for efficient API usage.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733741782.8853703}, "7394ed12a535b30122a9c096fe56b1020fc661a31b6d49cd1bee672e1843fec3": {"value": {"summary": "Base exception for documentation-related errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [], "description": "The `DocumentationError` class serves as the base exception for all errors related to documentation processing. It is designed to be inherited by more specific exception classes that handle particular documentation error scenarios.", "metadata": {}, "complexity": 0, "validation_status": true, "validation_errors": []}, "timestamp": 1733741784.6849015}, "16b532fdd654a681192955a5b2af783c7357f30144c783b48f6bf03552bf6017": {"value": {"summary": "Handles interaction with the Gemini model API.", "args": [{"name": "api_key", "type": "str", "description": "The API key used for authenticating requests to the Gemini model API."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL of the Gemini model API."}], "returns": {"type": "GeminiModel", "description": "An instance of the GeminiModel class initialized with the provided API key and endpoint."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised when there is an issue with the HTTP request to the Gemini model API."}, {"exception": "json.JSONDecodeError", "description": "Raised when the response from the API cannot be decoded into JSON format."}], "description": "The `GeminiModel` class is responsible for interfacing with the Gemini model API to perform tasks such as generating documentation and calculating token counts. It utilizes asynchronous HTTP requests to communicate with the API, allowing for efficient and non-blocking operations. The class also includes methods to generate prompts tailored for the Gemini model, ensuring that the input data is structured correctly for processing by the API.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741788.73055}, "a123ff1bf6d33baab2ac08c8283fc06926b2e5f12dc526d28fbff03566c6baa6": {"value": {"summary": "API request model for documentation generation.", "args": [{"name": "file_paths", "type": "List[str]", "description": "A list of file paths to be processed for documentation generation."}, {"name": "skip_types", "type": "Optional[List[str]]", "description": "A list of file types to skip during processing."}, {"name": "project_info", "type": "Optional[str]", "description": "Additional information about the project."}, {"name": "style_guidelines", "type": "Optional[str]", "description": "Style guidelines to be followed during documentation generation."}, {"name": "safe_mode", "type": "Optional[bool]", "description": "Flag indicating whether to operate in safe mode."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project."}, {"name": "provider", "type": "str", "description": "The AI provider to use for documentation generation, default is 'azure'."}, {"name": "max_concurrency", "type": "Optional[int]", "description": "Maximum number of concurrent operations allowed."}, {"name": "priority", "type": "Optional[str]", "description": "Priority level for the task, default is 'normal'."}, {"name": "callback_url", "type": "Optional[str]", "description": "URL to send callbacks to upon task completion."}], "returns": {"type": "DocumentationRequest", "description": "An instance of DocumentationRequest with validated attributes."}, "raises": [{"exception": "ValueError", "description": "Raised if the provider is not one of the valid options ('azure', 'gemini', 'openai') or if the priority is not 'low', 'normal', or 'high'."}], "description": "The DocumentationRequest class is an API request model that defines the structure and validation logic for requests to generate documentation. It includes attributes such as file paths, project information, style guidelines, and provider details. The class also provides validation methods to ensure the provider and priority values are within acceptable ranges.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741795.9477112}, "b05e9fe0649d8f116e0cd32a90cdcf95517ce1b24cf9e412d92eaf4b7e509a80": {"value": {"summary": "Enumeration of supported tokenizer models.", "args": [], "returns": {"type": "TokenizerModel", "description": "An enumeration of supported tokenizer models."}, "raises": [], "description": "The `TokenizerModel` class is an enumeration that defines the supported tokenizer models for text processing. It provides a set of predefined constants that represent different models used in tokenization tasks. This enumeration helps in managing and selecting the appropriate tokenizer model for various operations within the tokenization process.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733741798.288952}, "6819ad4d6981ea27e824a27938fe2a299931e2232f1cc68e89ea6dfbb747dae2": {"value": {"summary": "Manages and renders templates using Jinja2.", "args": [{"name": "template_dir", "type": "str", "description": "The directory where template files are stored. Defaults to 'templates'."}], "returns": {"type": "None", "description": "This constructor initializes the TemplateManager instance with the specified template directory."}, "raises": [{"exception": "FileNotFoundError", "description": "Raised when a custom template file does not exist."}, {"exception": "ValueError", "description": "Raised when an invalid template name is requested."}, {"exception": "Exception", "description": "Raised when there is an error loading or rendering a template."}], "description": "The TemplateManager class is responsible for managing templates stored in a specified directory and rendering them using the Jinja2 templating engine. It supports caching of templates to improve performance and allows for the addition of custom templates at runtime. The class handles asynchronous operations to fetch and render templates, making it suitable for use in asynchronous applications.", "metadata": {}, "complexity": 9, "validation_status": true, "validation_errors": []}, "timestamp": 1733741801.2301967}, "77d7bb9d5413285e715dcaa6c8e005a0fb4d8dd03bbd3f74c8c5a8155aea5c53": {"value": {"summary": "Configuration model for AI providers.", "args": [{"name": "name", "type": "str", "description": "The name of the AI provider."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL for the AI provider's API."}, {"name": "api_key", "type": "str", "description": "The API key used to authenticate with the AI provider."}, {"name": "deployment_name", "type": "Optional[str]", "description": "The name of the deployment, if applicable."}, {"name": "api_version", "type": "Optional[str]", "description": "The version of the API to use."}, {"name": "model_name", "type": "Optional[str]", "description": "The name of the model to use."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens to generate."}, {"name": "temperature", "type": "float", "description": "The sampling temperature, which must be between 0 and 1."}, {"name": "max_retries", "type": "int", "description": "The maximum number of retries for API requests."}, {"name": "retry_delay", "type": "float", "description": "The delay between retries in seconds."}, {"name": "cache_enabled", "type": "bool", "description": "Flag to enable or disable caching."}, {"name": "timeout", "type": "float", "description": "The timeout for API requests in seconds."}, {"name": "chunk_overlap", "type": "int", "description": "The number of overlapping tokens between chunks."}, {"name": "min_chunk_size", "type": "int", "description": "The minimum size of a chunk in tokens."}, {"name": "max_parallel_chunks", "type": "int", "description": "The maximum number of chunks to process in parallel."}], "returns": {"type": "ProviderConfig", "description": "An instance of the ProviderConfig class with validated and initialized attributes."}, "raises": [{"exception": "ValueError", "description": "Raised when the `temperature` is not between 0 and 1."}, {"exception": "ValueError", "description": "Raised when `max_tokens` is not between 1 and 8192."}], "description": "The `ProviderConfig` class is a configuration model designed to manage settings for AI providers. It inherits from the `BaseModel` class of the `pydantic` library, providing data validation and settings management for AI services. This model includes attributes such as `name`, `endpoint`, `api_key`, and others, which are essential for configuring AI provider connections. It also includes methods for validating specific attributes like `temperature` and `max_tokens`. These validations ensure that the configuration values are within acceptable ranges, thus preventing runtime errors due to misconfigurations.", "metadata": {}, "complexity": 4, "validation_status": true, "validation_errors": []}, "timestamp": 1733741808.7246826}, "479c490f2e58567ffc170d1dd8ffc5099f8d9cd554b2b76708dd245f77feb861": {"value": {"summary": "Calculates total tokens for the prompt content using TokenManager.", "args": [{"name": "base_info", "type": "str", "description": "Project and style information."}, {"name": "context", "type": "str", "description": "Related code or documentation that provides context."}, {"name": "chunk_content", "type": "str", "description": "Content of the chunk that is being documented."}, {"name": "schema", "type": "str", "description": "JSON schema representing the structure of the data."}], "returns": {"type": "int", "description": "The total number of tokens counted from the input strings."}, "raises": [{"exception": "TokenManagerError", "description": "If an error occurs within the TokenManager during token counting."}], "description": "This function calculates the total number of tokens for a given prompt content by utilizing the TokenManager utility. It takes in four strings: base information about the project, contextual information, the content of the chunk being documented, and a JSON schema. It iterates over these strings, counts the tokens for each using TokenManager, and accumulates the total token count.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741811.8903034}, "1ffa0cac032e66db021badab4485cfae4be6216cfafe0f537d7f0eb8ec61413d": {"value": {"summary": "Handles API interactions with error handling and rate limiting.", "args": [{"name": "session", "type": "aiohttp.ClientSession", "description": "An instance of aiohttp.ClientSession used to perform HTTP requests asynchronously."}, {"name": "config", "type": "ProviderConfig", "description": "Configuration object containing provider-specific settings like API keys, endpoints, and retry limits."}, {"name": "semaphore", "type": "asyncio.Semaphore", "description": "Semaphore to limit the number of concurrent API requests."}, {"name": "metrics_manager", "type": "MetricsManager", "description": "Manager for recording metrics related to API calls, such as success rates and latencies."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [{"exception": "ValueError", "description": "Raised when an unsupported provider is specified for an API request."}, {"exception": "Exception", "description": "Handles general exceptions during API requests and retries."}], "description": "The `APIHandler` class is responsible for managing interactions with various AI service providers, such as Azure, Gemini, and OpenAI. It implements error handling, rate limiting, and retry mechanisms to ensure robust API communication. The class uses asynchronous methods to perform API requests and manage rate limits efficiently. It also records metrics related to API calls, such as latency and token usage, for performance monitoring.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733741817.0957158}, "d72d3a4a1ce598918a68f6e50d5aa6717df866317ef817a9d141ced4664a216e": {"value": {"summary": "Main module for generating and inserting docstrings into code repositories.", "args": [{"name": "repo_path", "type": "str", "description": "Path to the code repository to be processed."}, {"name": "config", "type": "str", "description": "Path to the configuration file, defaulting to 'config.json'."}, {"name": "provider", "type": "str", "description": "AI provider to use, options are 'azure', 'gemini', or 'openai', defaulting to 'azure'."}, {"name": "concurrency", "type": "int", "description": "Number of concurrent requests to be made, default is 5."}, {"name": "skip_types", "type": "str", "description": "Comma-separated list of file extensions to skip during processing."}, {"name": "project_info", "type": "str", "description": "Additional information about the project."}, {"name": "style_guidelines", "type": "str", "description": "Documentation style guidelines to be followed."}, {"name": "safe_mode", "type": "bool", "description": "Flag to run the process in safe mode where no files are modified."}, {"name": "log_level", "type": "str", "description": "Logging level for the process, default is 'INFO'."}, {"name": "schema", "type": "str", "description": "Path to the function schema JSON file, default is 'schemas/function_schema.json'."}, {"name": "doc_output_dir", "type": "str", "description": "Directory where the generated documentation files will be saved, default is 'documentation'."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project, required argument."}], "returns": {"type": "None", "description": "This script does not return a value; it performs file processing and logging."}, "raises": [{"exception": "SystemExit", "description": "Raised when there is a critical error that requires the program to exit, such as unsupported provider or failed logging setup."}, {"exception": "Exception", "description": "General exception for any unhandled errors during the execution of the main function."}], "description": "This module provides functionality to generate and insert docstrings into code repositories using AI models from Azure, Gemini, or OpenAI. It parses command-line arguments to configure the process, including selecting the AI provider, setting concurrency levels, and specifying paths for configuration and output. The module supports safe mode operation where no files are modified and logs the process at a specified logging level. It validates the provider configuration and initializes the appropriate model client. The main function orchestrates loading configurations, schemas, and file paths, and utilizes the DocumentationProcessManager to process files asynchronously.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741825.6649752}, "a9f8a0f786ea6bee09b54990d390b39c4a4e05e41290c7b424331d1231f5dedc": {"value": {"summary": "Handles interaction with the Azure OpenAI API.", "args": [{"name": "api_key", "type": "str", "description": "API key for authenticating requests to the Azure OpenAI API."}, {"name": "endpoint", "type": "str", "description": "Base URL of the Azure OpenAI API endpoint."}, {"name": "deployment_name", "type": "str", "description": "Name of the specific deployment to be used within the Azure OpenAI service."}, {"name": "api_version", "type": "str", "description": "Version of the Azure OpenAI API to be used."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised if there is an issue with the HTTP request to the Azure OpenAI API."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error decoding the JSON response from the API."}], "description": "The AzureModel class facilitates interactions with the Azure OpenAI API, providing methods to generate documentation, calculate token usage, and create structured prompts for API requests. It manages API credentials and endpoint configurations necessary for making requests to the Azure OpenAI service.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741830.426082}, "4764b4e8498ce3c8df22df659a747cff25fbaa69aeef1c06b9a24f4c66f739c6": {"value": {"summary": "Handles Python code analysis and transformation.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for validating the extracted code structure."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "An instance of MetricsAnalyzer used for handling and analyzing code metrics."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [], "description": "The `PythonHandler` class is responsible for analyzing Python code to extract its structure, inserting docstrings, and validating the code using pylint. It leverages various libraries to parse the code, calculate metrics, and ensure compliance with predefined schemas. The class is designed to work asynchronously where necessary, particularly when extracting code structure and metrics.", "metadata": {}, "complexity": 30, "validation_status": true, "validation_errors": []}, "timestamp": 1733741834.9679525}, "f97d8dae1ae1da7e9e712da013ce11f99a72b07ffac43ba464a2937da985062e": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of overlapping tokens allowed between consecutive chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class initialized with the specified max_tokens and overlap."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any general errors encountered during the chunk creation process."}], "description": "The `ChunkManager` class is responsible for managing the operations related to code chunking. It provides functionalities to create, split, and merge code chunks, particularly focusing on Python code. The class utilizes an AST (Abstract Syntax Tree) analysis for Python code to create context-aware chunks, ensuring that the chunks are manageable and meaningful. The class also supports simple line-based chunking for other programming languages. The chunking process is guided by a maximum token limit and an overlap parameter to ensure continuity between chunks.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733741908.1685193}, "fa1d6346f1916c5fe3dc3d88c96655a6cf71dd3aa04d80dc28d3c1256b9c4c7c": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code, represented as strings of module and function names."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the provided source code during parsing."}], "description": "The DependencyAnalyzer class is designed to analyze Python source code and identify dependencies by traversing the Abstract Syntax Tree (AST). It extends the ast.NodeVisitor class to visit various types of nodes in the AST, such as import statements, name nodes, and call nodes. By doing so, it collects a set of dependencies, which are the names of modules and functions that are imported and utilized within the code. This tool is particularly useful for understanding the external modules and functions a piece of Python code relies on, aiding in dependency management and code analysis.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741911.459894}, "ee001c92d991dd88fb49d82907beea0c13842fcfb9094ae5ca6491580cc3b1c7": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read. This can be a string or a Path object."}, {"name": "use_cache", "type": "bool", "description": "Determines whether to use the internal cache for storing and retrieving file contents. Defaults to True."}, {"name": "encoding", "type": "str", "description": "The encoding to use for reading the file. Defaults to 'utf-8'."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs during reading."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised if the file cannot be decoded using the specified encoding."}, {"exception": "Exception", "description": "Raised for any other errors that occur during file reading."}], "description": "The `FileHandler` class provides methods to handle file operations asynchronously with caching and error handling. It is designed to efficiently read files by utilizing an internal cache to store file contents, reducing the need for repeated disk access. This class is particularly useful in scenarios where multiple reads of the same file are expected, and performance is a concern. The class also includes mechanisms to handle encoding errors gracefully, ensuring robust file reading even in the presence of unexpected file encodings.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741915.1305656}, "99251ea070c19d49a6735c3226383a155eb95675959ab9a824d148ec9d40478c": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [{"exception": "MetricsCalculationError", "description": "Raised when a general metrics calculation error occurs."}], "description": "The `MetricsCalculationError` class is a custom exception that serves as the base exception for errors encountered during metrics calculation. It is intended to be used as a parent class for more specific exceptions related to metrics processing, providing a clear and consistent way to handle errors in the metrics module.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733741921.339946}, "9b4dbe00e059f43d0abb45d8987b13d6428dba0081bafae2fab07302fa9ce005": {"value": {"summary": "Raised when a requested chunk cannot be found.", "args": [], "returns": {"type": "None", "description": "This exception does not return a value."}, "raises": [], "description": "The `ChunkNotFoundError` exception is used to signal that a specific code chunk requested by the user cannot be located within the current context or data structure. This exception helps in identifying issues related to missing or incorrectly referenced code chunks, ensuring that the system can handle such cases gracefully.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733741925.788246}, "3a768d6d30f472196d808e1847c1e049abcd464c8ac09372237144e7d86eb808": {"value": {"summary": "A Python module for interacting with OpenAI's GPT-4o model.", "args": [{"name": "api_key", "type": "str", "description": "The API key for authenticating with OpenAI's services."}, {"name": "base_info", "type": "str", "description": "Basic information to be included in the prompt."}, {"name": "context", "type": "str", "description": "Contextual information to guide the model's response."}, {"name": "chunk_content", "type": "str", "description": "Content to be processed by the model."}, {"name": "schema", "type": "Dict[str, Any]", "description": "A dictionary representing the schema to be included in the prompt."}, {"name": "prompt", "type": "List[Dict[str, str]]", "description": "A list of dictionaries representing the structured prompt for the model."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens to be generated in the response."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class."}, "raises": [{"exception": "openai.error.OpenAIError", "description": "If there is an error in the OpenAI API request."}, {"exception": "ValueError", "description": "If the input prompt is not in the expected format."}], "description": "The openai_model module provides a class, OpenAIModel, that facilitates interaction with OpenAI's GPT-4o model. It includes methods to generate structured prompts, calculate token usage, and generate documentation using the model. It requires an API key for authentication with OpenAI's services.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733741930.5209775}, "a8156dbb0f5532f4d88be5565fe1344cce6dbd7cacb689357ec2f6ec1e1d5cc4": {"value": {"summary": "Base exception for documentation-related errors.", "args": [], "returns": {"type": "None", "description": "This class does not return a value."}, "raises": [], "description": "This class serves as the base exception for all errors related to documentation processing. It inherits from the built-in Exception class and can be used to catch all documentation-specific exceptions in the module.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733741932.061513}, "b1793e6e158450d8de140ab1205d9a17bc444a2c1af12410e5fd39fe9b6850ee": {"value": {"summary": "Handles interaction with the Gemini model for token counting and documentation generation.", "args": [{"name": "api_key", "type": "str", "description": "API key for authenticating with the Gemini model API."}, {"name": "endpoint", "type": "str", "description": "Endpoint URL for the Gemini model API."}], "returns": {"type": "None", "description": "This is an initializer method, so it does not return a value."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised when there is an issue with the HTTP request to the Gemini API."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error decoding the JSON response from the Gemini API."}], "description": "The `GeminiModel` class provides methods to interact with the Gemini model API. It is designed to facilitate the generation of documentation and the calculation of token counts required for processing requests. The class manages API communication, including sending prompts and receiving generated documentation. It also constructs prompts and calculates token counts using the provided content and schema.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741936.1495001}, "d114caf7ae3f6423dd55dbdc90c069a73fb602a5268d6c9510b11487b3f7e4b5": {"value": {"summary": "API request model for documentation generation.", "args": [{"name": "file_paths", "type": "List[str]", "description": "A list of file paths to be processed for documentation generation."}, {"name": "skip_types", "type": "Optional[List[str]]", "description": "A list of file types to be skipped during processing."}, {"name": "project_info", "type": "Optional[str]", "description": "Information about the project for which documentation is being generated."}, {"name": "style_guidelines", "type": "Optional[str]", "description": "Style guidelines to be followed during documentation generation."}, {"name": "safe_mode", "type": "Optional[bool]", "description": "Flag indicating whether to run in safe mode, which may restrict certain operations."}, {"name": "project_id", "type": "str", "description": "A unique identifier for the project."}, {"name": "provider", "type": "str", "description": "The AI provider to be used for documentation generation. Must be one of 'azure', 'gemini', or 'openai'."}, {"name": "max_concurrency", "type": "Optional[int]", "description": "The maximum number of concurrent tasks allowed during processing."}, {"name": "priority", "type": "Optional[str]", "description": "The priority level of the request, which can be 'low', 'normal', or 'high'."}, {"name": "callback_url", "type": "Optional[str]", "description": "A URL to be called upon completion of the documentation generation process."}], "returns": {"type": "DocumentationRequest", "description": "An instance of the DocumentationRequest class."}, "raises": [{"exception": "ValueError", "description": "Raised if the provider is not one of the valid options ('azure', 'gemini', 'openai') or if the priority is not 'low', 'normal', or 'high'."}], "description": "The DocumentationRequest class serves as a model for API requests related to the documentation generation process. It includes various attributes that define the parameters for the request, such as file paths, project information, provider, and concurrency settings. The class also includes validation methods to ensure that the provider and priority values are within acceptable ranges.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741947.795954}, "504dbf4da343a699fb576316c97cc5709f499ae4fcb3e688a673496d8322219e": {"value": {"summary": "Enumeration of supported tokenizer models.", "args": [], "returns": {"type": "TokenizerModel", "description": "An enumeration of supported tokenizer models."}, "raises": [], "description": "The `TokenizerModel` class is an enumeration that defines the supported tokenizer models for text processing. It includes models such as GPT-4, GPT-3, and Codex, each associated with a specific encoding name used for tokenization purposes. This enumeration is primarily used to specify which tokenizer model should be used in various tokenization operations.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733741949.7834773}, "dc772cb071c08fd8fb3a942d5315079492c685d2e92657062871a691dc9131f2": {"value": {"summary": "Manages Jinja2 templates for rendering.", "args": [{"name": "template_dir", "type": "str", "description": "Directory containing the templates."}, {"name": "template_name", "type": "str", "description": "Name of the template to retrieve or render."}, {"name": "context", "type": "dict", "description": "Dictionary of variables to pass to the template for rendering."}, {"name": "name", "type": "str", "description": "Name to reference the custom template."}, {"name": "template_path", "type": "str", "description": "Path to the custom template file."}], "returns": {"type": "None", "description": "This class does not return a value directly but provides methods to get and render templates."}, "raises": [{"exception": "ValueError", "description": "Raised when the template name is invalid."}, {"exception": "FileNotFoundError", "description": "Raised when the specified template file does not exist."}, {"exception": "Exception", "description": "Raised when there is an error loading or rendering a template."}], "description": "The TemplateManager class provides functionality to manage and render Jinja2 templates. It allows users to load templates from a specified directory, cache them for efficient reuse, and render them with given context data. It also supports adding custom templates dynamically. This class is particularly useful for applications that need to generate dynamic content based on templates, such as documentation generators or content management systems.", "metadata": {}, "complexity": 9, "validation_status": true, "validation_errors": []}, "timestamp": 1733741956.3823845}, "a7ab29131435989cd5119b10615b5baa00a5d4bae0502a49a02955bdcf652d7c": {"value": {"summary": "Configuration model for AI providers.", "args": [{"name": "name", "type": "str", "description": "The name of the AI provider."}, {"name": "endpoint", "type": "str", "description": "The API endpoint for the AI provider."}, {"name": "api_key", "type": "str", "description": "The API key for authenticating with the AI provider."}, {"name": "deployment_name", "type": "Optional[str]", "description": "The deployment name for the AI provider, if applicable."}, {"name": "api_version", "type": "Optional[str]", "description": "The API version to use with the AI provider."}, {"name": "model_name", "type": "Optional[str]", "description": "The name of the model to use with the AI provider."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed per request."}, {"name": "temperature", "type": "float", "description": "The temperature setting for controlling randomness in model output."}, {"name": "max_retries", "type": "int", "description": "The maximum number of retry attempts for failed requests."}, {"name": "retry_delay", "type": "float", "description": "The delay between retry attempts, in seconds."}, {"name": "cache_enabled", "type": "bool", "description": "Flag indicating whether caching is enabled."}, {"name": "timeout", "type": "float", "description": "The timeout duration for requests, in seconds."}, {"name": "chunk_overlap", "type": "int", "description": "The overlap size between chunks when processing large inputs."}, {"name": "min_chunk_size", "type": "int", "description": "The minimum size of chunks when processing large inputs."}, {"name": "max_parallel_chunks", "type": "int", "description": "The maximum number of chunks to process in parallel."}], "returns": {"type": "ProviderConfig", "description": "An instance of the ProviderConfig class with validated configuration settings."}, "raises": [{"exception": "ValueError", "description": "Raised if the temperature is not between 0 and 1, or if max_tokens is not between 1 and 8192."}], "description": "The `ProviderConfig` class is a configuration model for AI providers, leveraging Pydantic's `BaseModel` to ensure data validation and type checking. It defines various attributes related to AI provider configurations, such as endpoint, API key, and model parameters. The class includes validators for specific fields to ensure valid input values. This class is intended to be used for managing and validating configuration settings for AI providers, supporting both file-based and environment variable-based configurations.", "metadata": {}, "complexity": 4, "validation_status": true, "validation_errors": []}, "timestamp": 1733741962.8825366}, "a17af71dfd4fb81a8fc39954e9e4d46f009d7a08687af12c539b1a110ec3fbab": {"value": {"summary": "Calculate total tokens for prompt content.", "args": [{"name": "base_info", "type": "str", "description": "Project and style information."}, {"name": "context", "type": "str", "description": "Related code or documentation context."}, {"name": "chunk_content", "type": "str", "description": "Content of the chunk being documented."}, {"name": "schema", "type": "str", "description": "JSON schema representing the structure."}], "returns": {"type": "int", "description": "The total token count for the provided content."}, "raises": [{"exception": "TokenManagerError", "description": "Raised if there is an error in counting tokens using TokenManager."}], "description": "This function calculates the total number of tokens for given prompt content using the TokenManager utility. It takes into account the base information, context, chunk content, and schema, and sums up the token counts for each of these components.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741968.6347296}, "ba833227eae808e5c10d2390a7e70317264ae0fdb625307f585f0cc0486ab64e": {"value": {"summary": "Handles API interactions with error handling and rate limiting.", "args": [{"name": "session", "type": "aiohttp.ClientSession", "description": "An instance of aiohttp.ClientSession used for making asynchronous HTTP requests."}, {"name": "config", "type": "ProviderConfig", "description": "Configuration settings for the API provider, including retry limits and rate limit settings."}, {"name": "semaphore", "type": "asyncio.Semaphore", "description": "A semaphore to limit the number of concurrent API requests."}, {"name": "metrics_manager", "type": "MetricsManager", "description": "An instance of MetricsManager to record metrics related to API calls."}], "returns": {"type": "None", "description": "This class does not return a value upon initialization."}, "raises": [{"exception": "Exception", "description": "General exception raised during API interactions."}, {"exception": "ValueError", "description": "Raised when an unsupported provider is specified."}], "description": "The `APIHandler` class is responsible for managing interactions with various API providers. It includes mechanisms for error handling and rate limiting to ensure reliable and efficient API calls. The class supports multiple providers such as Azure, Gemini, and OpenAI, and uses an asynchronous approach to handle requests. It also tracks API call metrics and manages rate limits using tokens.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733741977.6120534}, "36e2e9d44ddd970ca2fe43e68ef11f367c474634cce532b70ad3f75c1e5610c7": {"value": {"summary": "Main module for generating and inserting docstrings using AI models.", "args": [], "returns": {"type": "None", "description": "This module is executed as a script and does not return any value."}, "raises": [{"exception": "SystemExit", "description": "Raised when the script encounters a critical error and needs to exit, such as unsupported provider or failed logging setup."}, {"exception": "Exception", "description": "Catches any unhandled exceptions during the execution of the main function."}], "description": "This module provides functionality to generate and insert docstrings into a code repository using AI models from Azure, Gemini, or OpenAI. It parses command-line arguments to configure the process, loads necessary configurations and schemas, and manages the documentation generation process asynchronously. The module supports concurrency, safe mode, and custom logging configurations.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733741983.7868128}, "46336f5f394fc7fc13ab8b606674bda4c1365edca18be9cc889813c293d6d264": {"value": {"summary": "Handles interaction with the Azure OpenAI API.", "args": [{"name": "api_key", "type": "str", "description": "The API key used for authenticating requests to the Azure OpenAI API."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL for the Azure OpenAI API."}, {"name": "deployment_name", "type": "str", "description": "The name of the deployment to be used in the API requests."}, {"name": "api_version", "type": "str", "description": "The version of the API to be used."}], "returns": {"type": "None", "description": "This class does not return a value upon initialization."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised when there is an issue with the HTTP request to the Azure API."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error decoding the JSON response from the Azure API."}], "description": "The `AzureModel` class provides methods to interact with the Azure OpenAI API. It facilitates generating documentation by sending requests to the API, calculating token counts for prompts, and generating structured prompts for API requests. The class requires an API key, endpoint, deployment name, and API version for initialization.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733741990.684446}, "87fca67aa4f1782be05f844f052d130260901058abfdc019e37e7472b3d38fe9": {"value": {"summary": "Handles Python code analysis and docstring insertion.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "A dictionary representing the function schema to validate against."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "An instance of `MetricsAnalyzer` used to analyze and store code metrics."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [], "description": "The `PythonHandler` class is responsible for analyzing Python code to extract its structure, calculate code metrics, and insert docstrings. It extends the `BaseHandler` class and utilizes various tools and libraries such as `ast`, `jsonschema`, `radon`, and `pylint` to perform these tasks. The class provides methods to parse Python code, validate it against a predefined schema, calculate complexity and other metrics, and insert or update docstrings in the code. It also includes error handling and logging mechanisms to ensure robust operation.", "metadata": {}, "complexity": 30, "validation_status": true, "validation_errors": []}, "timestamp": 1733741995.8173919}, "ca59aee129c85c66b74cea589683834643789f99f987d3fb32c568f070b347a7": {"value": {"summary": "Handler for the CSS programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "CSSHandler", "description": "An instance of the CSSHandler class."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when the external JavaScript script execution fails."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error in parsing JSON output from the JavaScript scripts."}, {"exception": "Exception", "description": "Raised for any unexpected errors during the execution of methods."}], "description": "The `CSSHandler` class is designed to handle CSS code files by extending the `BaseHandler` abstract class. It provides methods to extract the structure of CSS code, insert documentation comments, and validate the code for correctness. The class utilizes external JavaScript scripts to parse and modify CSS code, ensuring a robust and efficient handling process.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733742002.006105}, "46985dca72ad3378a8409143f5b8c832637cc12a7c469f0d4fd8f872ca56efb9": {"value": {"summary": "Handles HTML code operations.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations, providing necessary configuration and guidelines for handling HTML code."}], "returns": {"type": "HTMLHandler", "description": "An instance of the HTMLHandler class initialized with the provided function schema."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when an external script execution fails."}, {"exception": "json.JSONDecodeError", "description": "Raised when the output from an external script cannot be parsed as JSON."}, {"exception": "FileNotFoundError", "description": "Raised when the 'tidy' tool is not installed or not found in the system PATH."}, {"exception": "Exception", "description": "Raised for any unexpected errors during script execution or HTML validation."}], "description": "The `HTMLHandler` class is designed to manage and manipulate HTML code files. It provides functionality to extract the structure of HTML code, insert documentation comments, and validate the correctness of HTML code using external tools. This class extends the `BaseHandler` abstract class, leveraging its interface to provide specialized handling for HTML content. The class utilizes external JavaScript scripts to perform parsing and comment insertion, and it uses the 'tidy' command-line tool for HTML validation.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733742008.5574253}, "f20e81bca21cb06aec53fb22cb691967f29d0381aee46604070728ea9545cc34": {"value": {"summary": "Handler for the Go programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "GoHandler", "description": "An instance of the GoHandler class."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when the external Go script encounters an error during execution."}, {"exception": "json.JSONDecodeError", "description": "Raised when the output from the Go script cannot be parsed as JSON."}, {"exception": "Exception", "description": "Raised for any unexpected errors during execution."}], "description": "The `GoHandler` class is designed to handle Go language code files. It provides methods to extract the structure of Go code, insert comments based on documentation, and validate the syntax of Go code. This class extends the `BaseHandler` abstract class and utilizes external Go scripts for parsing and modifying the code.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733742011.8362482}, "eb0d3e47dc552c193dea3fd30e5e00d06cacfae61ffd93a2ab88c85e4644a0bf": {"value": {"summary": "Handles Java code operations.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations, providing necessary guidelines for handling Java code."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [], "description": "The `JavaHandler` class is designed to handle various operations related to Java code, such as extracting the structure of Java classes, inserting Javadoc comments, and validating Java code for syntax correctness. It extends the `BaseHandler` class and utilizes external JavaScript scripts for parsing and modifying Java code. The class is initialized with a function schema that guides its operations.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733742031.294395}, "0df5e260cee80ab7d959c967ce45eb7226c4f092d4036eaf256c2a9d19ea59f3": {"value": {"summary": "Handler for the C++ programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "A schema defining functions for documentation generation."}], "returns": {"type": "CppHandler", "description": "An instance of the CppHandler class."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when a subprocess call for compiling or running C++ scripts fails."}, {"exception": "json.JSONDecodeError", "description": "Raised when JSON decoding of the parser output fails."}, {"exception": "FileNotFoundError", "description": "Raised when the 'g++' compiler is not found in the system PATH."}, {"exception": "Exception", "description": "Raised for any unexpected errors during code handling operations."}], "description": "The `CppHandler` class is designed to manage C++ code files by providing methods to extract code structure, insert documentation, and validate code syntax. It leverages external C++ scripts for parsing and modifying code, extending the functionality of the abstract `BaseHandler` class.", "metadata": {}, "complexity": 17, "validation_status": true, "validation_errors": []}, "timestamp": 1733742042.6639209}, "ff67d91f443388716914aa606dca1d892cfd3deda802de82e7003315827b95de": {"value": {"summary": "Enumerates JavaScript and TypeScript documentation styles.", "args": [], "returns": {"type": "JSDocStyle", "description": "An enumeration member representing a documentation style."}, "raises": [], "description": "The `JSDocStyle` class is an enumeration that defines two styles of documentation for JavaScript and TypeScript code: JSDoc and TSDoc. These styles are used to format comments and documentation within the code, providing a standardized way to describe the functionality, parameters, and return values of functions and classes.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742045.233397}, "2939d06a63437371773cb6bcaac453518db3b3d94539518cb07dfc6539341ab2": {"value": {"summary": "Handles different programming languages for documentation generation.", "args": [], "returns": {"type": "None", "description": "This module does not return anything as it defines functions for handling language-specific operations."}, "raises": [], "description": "This module provides utility functions to manage various programming languages within the documentation generation pipeline. It includes functionality to retrieve language-specific handlers and insert AI-generated docstrings or comments into source code. The module supports a range of languages including Python, Java, JavaScript, TypeScript, Go, C++, HTML, and CSS.", "metadata": {}, "complexity": 5, "validation_status": true, "validation_errors": []}, "timestamp": 1733742047.4953165}, "57f0a9c867c9ab11266b4481aac147e5353f58c23d5221b4dbe81c571030ba07": {"value": {"summary": "Initialize the language_functions package and provide a factory function for language handlers.", "args": [{"name": "language", "type": "str", "description": "The programming language of the source code. It is case-insensitive and should match one of the supported languages."}, {"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions, which is used by the language handler to process code."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "The metrics analyzer object used to analyze code metrics."}], "returns": {"type": "Optional[BaseHandler]", "description": "An instance of the corresponding language handler if the language is supported; otherwise, None."}, "raises": [{"exception": "None", "description": "The function does not raise exceptions but logs errors if the function schema is None or if the language is unsupported."}], "description": "The `__init__.py` module for the `language_functions` package initializes the package by importing necessary modules and functions. It provides a factory function, `get_handler`, to retrieve the appropriate language handler based on the specified programming language. This package is designed to handle various programming languages, including Python, Java, JavaScript, TypeScript, Go, C++, HTML, and CSS, by extracting code structures, inserting documentation comments, and validating code. The module also sets up logging for error and debug messages.", "metadata": {}, "complexity": 5, "validation_status": true, "validation_errors": []}, "timestamp": 1733742058.386091}, "d0cbbde74973fc9efc81885b0d634fa304eb251992925e625369fde23d9546c3": {"value": {"summary": "Manages code chunking operations for source code files.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between consecutive chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class initialized with specified token limits."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for general errors during the chunking process."}], "description": "The `ChunkManager` class is responsible for managing the operations related to dividing source code into manageable chunks. It supports chunking for different programming languages, with a focus on Python code using Abstract Syntax Tree (AST) analysis for context-aware chunking. The class provides methods to create, split, and merge code chunks, ensuring that each chunk adheres to a specified token limit. This is particularly useful for processing large code files in a way that maintains logical separation and context, which is essential for tasks like code analysis, refactoring, or documentation generation.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733742279.0670362}, "8c7248b7ec3b3ef087adc5922aa782fc5b39df56ac7b1725655aa943af75c1e9": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "code", "type": "str", "description": "The source code to be chunked."}, {"name": "file_path", "type": "str", "description": "The file path of the source code."}, {"name": "language", "type": "str", "description": "The programming language of the source code."}, {"name": "chunk", "type": "CodeChunk", "description": "The code chunk to be split or merged."}, {"name": "split_point", "type": "int", "description": "The line number at which to split the chunk."}, {"name": "chunk1", "type": "CodeChunk", "description": "The first code chunk to be merged."}, {"name": "chunk2", "type": "CodeChunk", "description": "The second code chunk to be merged."}], "returns": {"type": "List[CodeChunk]", "description": "A list of `CodeChunk` objects representing the divided code chunks."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the Python code."}, {"exception": "Exception", "description": "Raised for any general errors during chunk creation."}, {"exception": "ValueError", "description": "Raised if the split point is invalid or if chunks cannot be merged."}], "description": "The `ChunkManager` class is responsible for managing operations related to dividing code into manageable chunks. It supports creating chunks for different programming languages, with a focus on Python, using AST analysis for context awareness. The class also provides methods to split and merge chunks, ensuring that the chunks maintain logical and syntactical integrity. The chunking process is designed to respect token limits and overlaps, making it suitable for applications like code analysis, refactoring, and documentation generation.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733742332.675679}, "122e0d3de14a86b5a42dcfe5396711339ee92773a29fe7a4c1f338e11f658370": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code."}, "raises": [{"exception": "SyntaxError", "description": "Raised when the provided code has syntax errors."}], "description": "The DependencyAnalyzer class is designed to analyze Python source code to identify dependencies by utilizing the Abstract Syntax Tree (AST) module. It traverses the AST of the given code to detect import statements and usage of imported names. The class maintains a set of dependencies and imported names to provide a comprehensive view of the code's dependencies. This is particularly useful for understanding module dependencies, refactoring, or static code analysis.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733742335.0925145}, "0f1872bfdb209c3a38d2807cfb16c93cecabf3135d22d93785169466470d2d3d": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "Determines whether to use the internal cache for storing file content."}, {"name": "encoding", "type": "str", "description": "The encoding to be used when reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string if successful, otherwise None."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised when the file cannot be decoded with the specified encoding."}, {"exception": "Exception", "description": "Raised for any other errors encountered during file reading."}], "description": "The `FileHandler` class provides methods for reading files asynchronously with support for caching and error handling. It uses an internal cache to store file contents to reduce redundant file reads, especially useful for large files or frequently accessed files. The class also handles common file reading errors, such as encoding issues, by attempting to read the file with error replacement strategies. It is designed to work efficiently in an asynchronous environment using aiofiles for non-blocking file operations.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733742339.1030078}, "c15351d55367028b5f2f42c3ec681d3aae47bb564e04babf150a3ce4b83c6f40": {"value": {"summary": "Exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return a value as it is used for exception handling."}, "raises": [], "description": "The `MetricsCalculationError` class serves as a custom exception for handling errors that occur during the calculation of metrics within the module. It inherits from the base `Exception` class and does not add any additional functionality or attributes.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742342.9019535}, "c5e47a1c3f3856f7b14d9edbd9ee21a674ae66686e96225a729332d3af3710bc": {"value": {"summary": "Exception raised when a requested chunk cannot be found.", "args": [], "returns": {"type": "None", "description": "This exception does not return any value."}, "raises": [], "description": "This exception is raised when an operation attempts to access a code chunk that does not exist in the context manager's tree structure. This could occur if the chunk ID is incorrect or if the chunk has been removed from the context.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742345.1222167}, "29317778c85567dc1835195d4dad3c362302617017f4708bc666b6a95071ea61": {"value": {"summary": "Represents an interface to interact with OpenAI's GPT-4o model.", "args": [{"name": "api_key", "type": "str", "description": "The API key for authenticating with the OpenAI service."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class."}, "raises": [{"exception": "openai.error.AuthenticationError", "description": "Raised if the provided API key is invalid."}, {"exception": "openai.error.OpenAIError", "description": "Raised for general errors related to OpenAI API requests."}], "description": "The OpenAIModel class provides methods to generate structured prompts, calculate token counts, and fetch generated documentation using OpenAI's GPT-4o model. It is designed to facilitate the integration of OpenAI's language model capabilities into applications, allowing for dynamic prompt creation and response handling. The class requires an API key for authentication with the OpenAI service.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742348.5934343}, "51ad91afb8a4c473b4a5a7aad244e36f1f51aa8b718955890565929d725c9566": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between consecutive chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there's a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for general errors during chunk creation."}], "description": "The `ChunkManager` class provides functionality to manage and manipulate code chunks. It supports creating, splitting, and merging code chunks with context awareness, particularly for Python code. The class utilizes token management to ensure chunks do not exceed a specified token limit, allowing for overlap to maintain context between chunks. It can handle different programming languages, with specialized handling for Python using Abstract Syntax Tree (AST) analysis.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733742449.2793481}, "417010485615fc1804546fbd2e5fd057d1c9d1de1ba9b2fec5d6da5988219897": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze. It should be a string containing valid Python code."}], "returns": {"type": "Set[str]", "description": "A set of strings representing the dependencies found in the code. Each string is the name of a module or function that is imported and used in the code."}, "raises": [{"exception": "SyntaxError", "description": "Raised if the provided code contains syntax errors that prevent it from being parsed."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code to identify module dependencies. It utilizes Python's Abstract Syntax Tree (AST) to parse the code and extract information about imported modules and their usage. By visiting various nodes in the AST, such as import statements, names, and function calls, the class collects a set of dependencies that represent the modules and functions used within the code. This tool is useful for understanding code dependencies, refactoring, or preparing for deployment by identifying which modules are required.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733742452.6263745}, "03760ac855f3a84a3f1da5fc7035dd0465a266b2b9de94cdc230e7b45b6521b3": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "Flag indicating whether to use cached content if available."}, {"name": "encoding", "type": "str", "description": "The encoding to use for reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised when the file cannot be decoded using the specified encoding."}, {"exception": "Exception", "description": "Raised for any other errors encountered while reading the file."}], "description": "The `FileHandler` class provides methods for performing file operations asynchronously, with support for caching and error handling. It is designed to efficiently read file contents while minimizing redundant file reads through caching. The class also handles common file reading errors such as `UnicodeDecodeError` and logs relevant warnings and errors.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733742455.7566957}, "79333ff825e93a23a5bc7b1bee50dfdc2defe1740958332e941320f9e7eac509": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return a value as it is an exception class."}, "raises": [], "description": "The `MetricsCalculationError` class serves as a base exception for errors that occur during metrics calculation processes. It is used to signal issues specifically related to the computation or tracking of metrics within the module.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742457.4705162}, "90c39a71fdc8204eba0c6ca6f1a41a2860996c62a66354c9155b3601e9bc8d49": {"value": {"summary": "Raised when a requested chunk cannot be found.", "args": [], "returns": {"type": "None", "description": "This exception does not return a value."}, "raises": [], "description": "The ChunkNotFoundError is an exception that is raised when a specific code chunk, identified by its unique ID or path, cannot be located within the context management system. This exception is typically used to signal that an operation requiring a specific chunk cannot proceed because the chunk is missing from the hierarchical structure.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742461.7759628}, "4235a5cce288b983ce2135f731cab1f655b4b9321a40ef65e7a9591ec66cd5f9": {"value": {"summary": "Interacts with OpenAI's GPT-4o model for prompt generation and documentation.", "args": [{"name": "api_key", "type": "str", "description": "The API key used to authenticate with the OpenAI API."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class configured with the provided API key."}, "raises": [{"exception": "openai.error.AuthenticationError", "description": "Raised if the provided API key is invalid."}, {"exception": "openai.error.OpenAIError", "description": "Raised for other errors related to the OpenAI API."}], "description": "The OpenAIModel class provides methods to create structured prompts, calculate token counts, and generate documentation using OpenAI's GPT-4o model. It integrates with OpenAI's API using the provided API key and logs interactions for debugging purposes.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742464.6370642}, "2725b16b74a1b26686131e1d9a810c7ea7eaf6181477d7acf1917fc52c389a95": {"value": {"summary": "Base exception for documentation-related errors.", "args": [], "returns": {"type": "None", "description": "This class does not return a value as it is an exception."}, "raises": [], "description": "The `DocumentationError` class serves as the base exception for errors related to documentation processing. It is intended to be subclassed by more specific exceptions that handle various documentation-related issues.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742466.3357236}, "c085f1ad3f8dcad40a8c8acbb62b9584b67f3707509cf4f47da9cb1848cdcf58": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk. Default is 4096."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between consecutive chunks to maintain context. Default is 200."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class, initialized with specified or default parameters for maximum tokens and overlap."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any general errors encountered during the chunking process."}], "description": "The `ChunkManager` class is responsible for managing the operations related to code chunking. It provides functionality to create, split, and merge code chunks, particularly focusing on handling Python code with context awareness using Abstract Syntax Trees (AST). The class utilizes a `TokenManager` to manage tokenization and ensure that chunks do not exceed a specified token limit, allowing for overlap between chunks to maintain context. This is particularly useful in scenarios where code needs to be processed in smaller, manageable pieces without losing the semantic meaning across chunks.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733742537.6803923}, "4928b5318739bfdd5bf7bbbd62ebbf2e7634035c35d2f090f4c481d575c3c1e2": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze, provided as a string."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the provided source code."}], "description": "The DependencyAnalyzer class utilizes the Abstract Syntax Tree (AST) module to analyze dependencies in a Python source file. It identifies and records imported modules and functions, tracking their usage within the code. The class is designed to parse the source code, visit relevant nodes, and collect dependencies based on import statements and function calls. It provides a systematic way to understand the dependencies of a Python file, which can be useful for code analysis, refactoring, or dependency management.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733742540.324542}, "5075fd00b1938c491a99288ddecde72d70f28adf4d2bd16471c4254828456ac4": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read. This can be a string or a Path object."}, {"name": "use_cache", "type": "bool", "description": "A flag indicating whether to use the cache for reading the file. If True, the method will attempt to retrieve the file content from the cache before reading from the file system."}, {"name": "encoding", "type": "str", "description": "The encoding to use when reading the file. Defaults to 'utf-8'."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string if successful, or None if an error occurs during reading."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised when the file cannot be decoded using the specified encoding."}, {"exception": "Exception", "description": "Raised for any other errors that occur during the file reading process."}], "description": "The `FileHandler` class provides methods for asynchronous file reading with caching capabilities. It is designed to handle file operations efficiently by using an internal cache to store file contents, reducing the need for repeated file I/O operations. The class also includes error handling mechanisms to manage common file reading issues, such as encoding errors. The caching mechanism is thread-safe, utilizing a lock to ensure consistency when accessing the cache. Additionally, the class supports a maximum cache size to prevent excessive memory usage, and it employs a thread pool executor to manage asynchronous tasks.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733742545.4980738}, "95b0bbb70a0ed5499048de26442286f8bcacc70e730839f292f825a793e8e685": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This exception does not return any value."}, "raises": [], "description": "The `MetricsCalculationError` class serves as a custom exception for errors encountered during metrics calculations. It inherits from the base `Exception` class in Python, providing a specific error type for handling issues related to metrics computations within the module.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742547.1206608}, "b976c297f0373b9a96ddd0355b45390be6476c3bef427c9fdae3f4062c5b9373": {"value": {"summary": "Exception raised when a requested chunk is not found.", "args": [], "returns": {"type": "None", "description": "This exception does not return a value."}, "raises": [], "description": "The `ChunkNotFoundError` class is a custom exception that is raised when a requested chunk cannot be found within the context management system. This exception is typically used to indicate that an operation requiring a specific chunk cannot proceed because the chunk does not exist.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742549.8720875}, "a94bddd71e6d6f91d6e0494a6a23a5d0aabc22d709cc06c5bfe6294fb864d006": {"value": {"summary": "Represents an interface to interact with OpenAI's GPT-4o model.", "args": [{"name": "api_key", "type": "str", "description": "The API key for authenticating requests to the OpenAI API."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class."}, "raises": [{"exception": "openai.error.AuthenticationError", "description": "Raised if the provided API key is invalid or unauthorized."}, {"exception": "openai.error.OpenAIError", "description": "Raised for general errors related to the OpenAI API."}], "description": "The OpenAIModel class provides methods to construct prompts, calculate token counts, and generate documentation using OpenAI's GPT-4o model. It requires an API key for authentication and leverages the OpenAI API to perform its operations. This class is designed to facilitate the integration of OpenAI's language model capabilities into applications, allowing for dynamic content generation based on structured prompts.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742555.610114}, "4dc3450758045ba8a2f757b8ba38e4488debf1cb3198585a6010aba5f9cc0524": {"value": {"summary": "Module for generating and writing documentation reports.", "args": [], "returns": {"type": "None", "description": "This module does not return any value directly as it is intended to be used as a script or imported module."}, "raises": [], "description": "The `write_documentation_report` module provides functionality to generate and write documentation reports in both JSON and Markdown formats. It includes classes for handling errors related to documentation, badge configuration and generation, Markdown formatting, and documentation generation. The module utilizes asynchronous file operations for writing documentation to ensure non-blocking execution. It also supports template-based documentation generation using Jinja2 templates.", "metadata": {}, "complexity": 0, "validation_status": true, "validation_errors": []}, "timestamp": 1733742558.220462}, "ec3c2670996c2af04831471997ad093d95efe4d4ecc671bad49271fd640fb858": {"value": {"summary": "Handles interaction with the Gemini model API.", "args": [{"name": "api_key", "type": "str", "description": "API key for authenticating with the Gemini model API."}, {"name": "endpoint", "type": "str", "description": "Endpoint URL for the Gemini model API."}], "returns": {"type": "None", "description": "This class does not return a value upon initialization."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised if there is an issue with the HTTP request to the Gemini API."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error decoding the JSON response from the Gemini API."}], "description": "The GeminiModel class provides methods to interact with the Gemini model API for generating documentation and calculating token usage. It includes functionalities to send prompts to the API, process responses, and manage token counts using a TokenManager. This class is designed to facilitate the integration of the Gemini model into applications that require automated documentation generation and token management.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733742570.562344}, "68996428a07def143548a734cd308663a5c0eb15fbc86cf4e6249af2a25ed0b0": {"value": {"summary": "API request model for documentation generation.", "args": [{"name": "file_paths", "type": "List[str]", "description": "List of file paths to be processed for documentation generation."}, {"name": "skip_types", "type": "Optional[List[str]]", "description": "List of file types to be skipped during processing."}, {"name": "project_info", "type": "Optional[str]", "description": "Information about the project for which documentation is being generated."}, {"name": "style_guidelines", "type": "Optional[str]", "description": "Guidelines to be followed for documentation style."}, {"name": "safe_mode", "type": "Optional[bool]", "description": "Flag to enable or disable safe mode during processing."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project."}, {"name": "provider", "type": "str", "description": "Name of the AI provider to be used for documentation generation."}, {"name": "max_concurrency", "type": "Optional[int]", "description": "Maximum number of concurrent processes allowed."}, {"name": "priority", "type": "Optional[str]", "description": "Priority level of the documentation generation task."}, {"name": "callback_url", "type": "Optional[str]", "description": "URL to be called upon task completion."}], "returns": {"type": "DocumentationRequest", "description": "An instance of the DocumentationRequest class with validated fields."}, "raises": [{"exception": "ValueError", "description": "Raised if the provider or priority fields contain invalid values."}], "description": "The DocumentationRequest class models the input data required for initiating a documentation generation process. It includes various parameters such as file paths, project information, and configuration settings for the AI provider. The class also validates the provider and priority fields to ensure they contain acceptable values.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733742575.2906246}, "f50a8740a941aaa8a0de8295e67b99e41400a9804d1d7b95cf2e7802c3db69ea": {"value": {"summary": "Enumeration of supported tokenizer models.", "args": [], "returns": {"type": "TokenizerModel", "description": "An enumeration member representing a tokenizer model."}, "raises": [], "description": "The `TokenizerModel` class is an enumeration that defines the supported tokenizer models used in the tokenization process. It provides a simple way to refer to different tokenizer configurations by name, ensuring consistency and clarity in the code. Each member of the enumeration corresponds to a specific tokenizer model configuration.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742577.173109}, "e0eb749ebe39e0389f314246eac321a4b0de99b77b169284afa679bfe4f1747c": {"value": {"summary": "Manage and render Jinja2 templates.", "args": [{"name": "template_dir", "type": "str", "description": "The directory containing the template files."}, {"name": "template_name", "type": "str", "description": "The name of the template to retrieve or render."}, {"name": "context", "type": "dict", "description": "A dictionary of variables to pass to the template for rendering."}, {"name": "name", "type": "str", "description": "The name to reference a custom template."}, {"name": "template_path", "type": "str", "description": "The file path to a custom template."}], "returns": {"type": "Template or str", "description": "Returns a Jinja2 Template object when retrieving a template, and a rendered string when rendering a template."}, "raises": [{"exception": "ValueError", "description": "Raised if the template name is invalid when retrieving a template."}, {"exception": "FileNotFoundError", "description": "Raised if the specified custom template file does not exist."}, {"exception": "Exception", "description": "Raised for any other errors during template loading or rendering."}], "description": "The TemplateManager class is responsible for managing Jinja2 templates within a specified directory. It supports loading, caching, and rendering templates asynchronously. Additionally, it allows the addition of custom templates at runtime. Templates are loaded from a specified directory and can be rendered with provided context data. The class ensures efficient template management by caching loaded templates and provides error handling for template loading and rendering operations.", "metadata": {}, "complexity": 9, "validation_status": true, "validation_errors": []}, "timestamp": 1733742581.4644175}, "138394c997dc6d842c12dffbf19ea000864334a9f55dd38f3286379d13cda3b6": {"value": {"summary": "Configuration model for AI providers.", "args": [{"name": "name", "type": "str", "description": "The name of the AI provider."}, {"name": "endpoint", "type": "str", "description": "The API endpoint for the AI provider."}, {"name": "api_key", "type": "str", "description": "The API key used to authenticate requests to the AI provider."}, {"name": "deployment_name", "type": "Optional[str]", "description": "The name of the deployment, if applicable."}, {"name": "api_version", "type": "Optional[str]", "description": "The version of the API being used."}, {"name": "model_name", "type": "Optional[str]", "description": "The name of the model being used."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a request. Defaults to 4096."}, {"name": "temperature", "type": "float", "description": "The sampling temperature for the model. Must be between 0 and 1. Defaults to 0.7."}, {"name": "max_retries", "type": "int", "description": "The maximum number of retries for failed requests. Defaults to 3."}, {"name": "retry_delay", "type": "float", "description": "The delay between retries in seconds. Defaults to 1.0."}, {"name": "cache_enabled", "type": "bool", "description": "Indicates whether caching is enabled. Defaults to True."}, {"name": "timeout", "type": "float", "description": "The timeout for requests in seconds. Defaults to 30.0."}, {"name": "chunk_overlap", "type": "int", "description": "The overlap size between chunks. Defaults to 200."}, {"name": "min_chunk_size", "type": "int", "description": "The minimum size of a chunk. Defaults to 100."}, {"name": "max_parallel_chunks", "type": "int", "description": "The maximum number of parallel chunks. Defaults to 3."}], "returns": {"type": "ProviderConfig", "description": "An instance of the ProviderConfig class with the specified settings."}, "raises": [{"exception": "ValueError", "description": "Raised when `temperature` is not between 0 and 1."}, {"exception": "ValueError", "description": "Raised when `max_tokens` is not between 1 and 8192."}], "description": "The `ProviderConfig` class is a configuration model used to define settings for AI providers. It extends the `BaseModel` class from Pydantic, allowing for data validation and settings management. This class holds various attributes related to AI provider configurations, such as API keys, endpoints, and model parameters. Additionally, it includes validation methods to ensure that certain attributes, like `temperature` and `max_tokens`, fall within specified ranges.", "metadata": {}, "complexity": 4, "validation_status": true, "validation_errors": []}, "timestamp": 1733742589.346948}, "5cc95523464fe60a96b6f45ba037287079ed6e1a3bd4559a1097f48281bebd96": {"value": {"summary": "Calculates total tokens for the prompt content using TokenManager.", "args": [{"name": "base_info", "type": "str", "description": "Project and style information that serves as part of the prompt content."}, {"name": "context", "type": "str", "description": "Related code or documentation that provides context for the prompt."}, {"name": "chunk_content", "type": "str", "description": "The actual content of the chunk being documented, which forms part of the prompt."}, {"name": "schema", "type": "str", "description": "A JSON schema represented as a string, which is included in the prompt content."}], "returns": {"type": "int", "description": "The total count of tokens for the combined prompt content."}, "raises": [{"exception": "TokenManagerError", "description": "Raised if there is an error in counting tokens using the TokenManager."}], "description": "This function calculates the total number of tokens required for a given set of prompt content components. It utilizes the TokenManager class to count the tokens for each component, which includes base information, context, chunk content, and schema. The function iterates over these components, retrieves the token count for each, and accumulates the total token count. This is useful for determining the size of the prompt in token-based systems, ensuring that the prompt does not exceed token limits.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733742593.5739346}, "cfa344875a3dd47c8751e4338d2d12d86ed5c256f96849e72570d53ade307031": {"value": {"summary": "Handles API interactions with error handling and rate limiting.", "args": [{"name": "session", "type": "aiohttp.ClientSession", "description": "An active aiohttp client session used for making HTTP requests."}, {"name": "config", "type": "ProviderConfig", "description": "Configuration settings for the API provider, including rate limits and retry policies."}, {"name": "semaphore", "type": "asyncio.Semaphore", "description": "Semaphore used to limit the number of concurrent API requests."}, {"name": "metrics_manager", "type": "MetricsManager", "description": "Manager for recording metrics related to API calls."}], "returns": {"type": "None", "description": "This class does not return a value; it manages API interactions and state internally."}, "raises": [{"exception": "Exception", "description": "General exception raised for errors during API interactions."}, {"exception": "ValueError", "description": "Raised when an unsupported provider is specified."}], "description": "The APIHandler class is designed to manage interactions with various API providers, ensuring that requests are made efficiently and within rate limits. It includes mechanisms for handling errors and retrying requests when necessary. The class supports multiple providers and uses asynchronous operations to improve performance. It also tracks metrics related to API calls, such as latency and token usage.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733742598.113562}, "f6e0473e285e5479a5ee942ca2934ecb663705c4e1b062ff2397152fe6eba6ec": {"value": {"summary": "Main module for generating and inserting docstrings using AI models.", "args": [{"name": "repo_path", "type": "str", "description": "Path to the code repository where docstrings will be generated and inserted."}, {"name": "config", "type": "str", "description": "Path to the configuration file (config.json) containing settings for the documentation generation process."}, {"name": "provider", "type": "str", "description": "AI provider to use for generating docstrings. Options are 'azure', 'gemini', or 'openai'."}, {"name": "concurrency", "type": "int", "description": "Number of concurrent requests to make when generating docstrings."}, {"name": "skip_types", "type": "str", "description": "Comma-separated list of file extensions to skip during the documentation generation process."}, {"name": "project_info", "type": "str", "description": "Additional information about the project to be included in the documentation."}, {"name": "style_guidelines", "type": "str", "description": "Documentation style guidelines to follow when generating docstrings."}, {"name": "safe_mode", "type": "bool", "description": "Flag to run the script in safe mode, where no files are modified."}, {"name": "log_level", "type": "str", "description": "Logging level for the script's output."}, {"name": "schema", "type": "str", "description": "Path to the function schema file (function_schema.json) used for generating docstrings."}, {"name": "doc_output_dir", "type": "str", "description": "Directory where generated documentation files will be saved."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project being documented."}], "returns": {"type": "None", "description": "This script does not return a value but generates documentation files in the specified output directory."}, "raises": [{"exception": "SystemExit", "description": "Raised when the script encounters a critical error or when the user interrupts the process."}, {"exception": "Exception", "description": "Raised for any unhandled exceptions during the execution of the script."}], "description": "This module serves as the main entry point for a script that generates and inserts docstrings into code repositories using AI models from Azure, Gemini, or OpenAI. It parses command-line arguments, sets up logging, loads configurations, and manages the documentation generation process. The module supports concurrency and safe mode operations, allowing users to customize the documentation generation process according to their needs.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733742603.7915385}, "153b0a6db82a4738ea9b7c70b4ab68e18d745f51db8afdcce25de93006debe59": {"value": {"summary": "Handles interaction with the Azure OpenAI API.", "args": [{"name": "api_key", "type": "str", "description": "The API key for authenticating requests to the Azure OpenAI API."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL of the Azure OpenAI service."}, {"name": "deployment_name", "type": "str", "description": "The name of the deployment to be used in the API requests."}, {"name": "api_version", "type": "str", "description": "The version of the API to be used in the requests."}], "returns": {"type": "AzureModel", "description": "An instance of the AzureModel class initialized with the provided API key, endpoint, deployment name, and API version."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised if there is an error in the HTTP request to the Azure OpenAI API."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error decoding the JSON response from the Azure OpenAI API."}], "description": "The AzureModel class provides methods to interact with the Azure OpenAI API, enabling functionalities such as generating documentation, calculating token counts, and creating prompt structures. It manages the API request logic and handles the responses from the Azure OpenAI service. The class requires an API key, endpoint, deployment name, and API version to initialize and make requests to the Azure service.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733742607.3986993}, "816ab72de312834a7ea77955339596f4fb281860905bfe543a02ded6e03cf07b": {"value": {"summary": "Handles Python code structure extraction, docstring insertion, and validation.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "A dictionary representing the schema against which the code structure is validated."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "An instance of MetricsAnalyzer used for analyzing code metrics."}, {"name": "code", "type": "str", "description": "The Python code to be processed."}, {"name": "file_path", "type": "str", "description": "The path to the Python file being analyzed."}, {"name": "metrics", "type": "Optional[Dict[str, Any]]", "description": "Optional metrics data to use during structure extraction."}, {"name": "documentation", "type": "Dict[str, Any]", "description": "Documentation data used for inserting docstrings."}], "returns": {"type": "Dict[str, Any]", "description": "A dictionary containing the extracted code structure, including functions, classes, variables, constants, imports, and metrics."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the provided Python code."}, {"exception": "Exception", "description": "Raised for any other errors encountered during code processing."}], "description": "The PythonHandler class is responsible for analyzing Python code to extract its structure, insert docstrings, and validate the code using pylint. It leverages the Abstract Syntax Tree (AST) to parse the code and extract relevant information such as functions, classes, variables, and imports. It also calculates code complexity and validates the code structure against a predefined schema. Additionally, it can insert Google-style or NumPy-style docstrings into the code using AST manipulation.", "metadata": {}, "complexity": 30, "validation_status": true, "validation_errors": []}, "timestamp": 1733742613.4146032}, "c841902aab8fb78f040c099459ff98f6bf5f7ff9ad7ce69ec57e7d0be89bb2ee": {"value": {"summary": "Handles CSS code files for structure extraction, documentation insertion, and validation.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "CSSHandler", "description": "An instance of the CSSHandler class initialized with the provided function schema."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when an error occurs in the subprocess call to external scripts."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error decoding JSON output from scripts."}, {"exception": "FileNotFoundError", "description": "Raised when 'stylelint' is not installed or not found in the system PATH."}, {"exception": "Exception", "description": "Raised for any unexpected errors during execution."}], "description": "The `CSSHandler` class is designed to manage CSS code files by providing methods to extract the code structure, insert documentation comments, and validate the code using external tools. It extends the `BaseHandler` abstract class and relies on JavaScript scripts for parsing and modifying CSS code. The class is initialized with a function schema that guides the documentation generation process.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733742623.7065868}, "e7434b1a46c258e9c1abcb9ec043628744ced95f4e8c081dc800f12ab8c4dc3d": {"value": {"summary": "Handles HTML code operations.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations, providing necessary configurations and rules for handling HTML code."}], "returns": {"type": "None", "description": "This class does not return a value upon initialization."}, "raises": [], "description": "The `HTMLHandler` class is designed to manage and manipulate HTML code files. It extends the `BaseHandler` class and provides methods for extracting the structure of HTML code, inserting documentation as comments, and validating the correctness of the HTML code. The class utilizes external JavaScript scripts to perform these operations, ensuring accurate parsing and modification of HTML code.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733742629.4103343}, "c3f4b6244c195217380612a5951fa257cc3240a11eef9bfc2db943fbb4625f87": {"value": {"summary": "Handler for the Go programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [], "description": "The `GoHandler` class is designed to manage Go programming language code files. It provides methods to extract the code structure, insert documentation comments, and validate the Go code. The class relies on external Go scripts to parse and modify the code, ensuring that the operations are aligned with Go's syntax and semantics. It extends the `BaseHandler` abstract class, inheriting its interface and potentially other functionalities.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733742632.5046353}, "0ab8bcb078b7a9ad30b7ed51861903f6e534ccfec48bea3d605e5ffd6d29598d": {"value": {"summary": "Handles Java code operations.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations, providing necessary configurations and rules."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [], "description": "The JavaHandler class is responsible for managing Java code files. It provides methods to extract the structure of Java code, insert Javadoc comments, and validate the code syntax using external scripts and tools. The class extends the BaseHandler abstract class and utilizes JavaScript scripts for parsing and modifying Java code.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733742634.5779526}, "ae0117843a07262093d23675ca98f2d7601c9ea7b883419822eef7c9a326e858": {"value": {"summary": "Handler for the C++ programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}, {"name": "code", "type": "str", "description": "The source code to analyze or modify."}, {"name": "file_path", "type": "Optional[str]", "description": "The file path for code reference or validation. Defaults to None."}, {"name": "documentation", "type": "Dict[str, Any]", "description": "Documentation details obtained from AI for insertion into the code."}], "returns": {"type": "Dict[str, Any]", "description": "A detailed structure of the code components or the modified source code with inserted documentation."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised if there is an error running the external C++ scripts."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error parsing the JSON output from the C++ parser."}, {"exception": "FileNotFoundError", "description": "Raised if the 'g++' compiler is not installed or not found in PATH."}, {"exception": "Exception", "description": "Catches any unexpected errors during execution."}], "description": "The `CppHandler` class is designed to manage C++ code files by providing functionalities such as extracting code structure, inserting documentation comments, and validating the code for syntax correctness. It extends the `BaseHandler` abstract class, leveraging external C++ scripts for parsing and modifying the code. This class is particularly useful for automating documentation processes and ensuring code quality in C++ projects.", "metadata": {}, "complexity": 17, "validation_status": true, "validation_errors": []}, "timestamp": 1733742646.074325}, "ab1e7e6a0cf01ad2feef5d53a0a689997d02ce659680dba329e991ba5c421d9a": {"value": {"summary": "Enumeration for JavaScript and TypeScript documentation styles.", "args": [], "returns": {"type": "JSDocStyle", "description": "An enumeration member representing the documentation style."}, "raises": [], "description": "The `JSDocStyle` class is an enumeration that defines the documentation styles used in JavaScript and TypeScript code. It provides two styles: `JSDOC` for JavaScript and `TSDOC` for TypeScript. This enum is used to specify the format of docstrings to be inserted or processed within the code.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733742648.4566207}, "3da1ec30c432bb91fb72c221c9c6912c25fa7c7f8c06acc33d36a5ef6b5f294f": {"value": {"summary": "Retrieve the appropriate handler for a programming language.", "args": [{"name": "language", "type": "str", "description": "The programming language of the source code, such as 'python', 'java', 'javascript', etc."}, {"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions and their documentation structure."}], "returns": {"type": "Optional[BaseHandler]", "description": "An instance of the corresponding language handler or None if the language is unsupported."}, "raises": [{"exception": "None", "description": "This function does not raise exceptions directly but logs warnings and errors."}], "description": "This function serves as a factory to retrieve the appropriate handler for a given programming language. It matches the provided language with its corresponding handler class, such as PythonHandler for Python, JavaHandler for Java, etc. If the language is supported, it returns an instance of the handler initialized with the given function schema. In case the language is unsupported, it logs a warning and returns None.", "metadata": {}, "complexity": 5, "validation_status": true, "validation_errors": []}, "timestamp": 1733742653.213718}, "237d24e4a53b88b10c3e055ae4e191440ab5bc09e9777f6cdddc5f938c6d818f": {"value": {"summary": "Retrieve the appropriate language handler for code processing.", "args": [{"name": "language", "type": "str", "description": "The programming language of the source code to be processed."}, {"name": "function_schema", "type": "Dict[str, Any]", "description": "A dictionary defining the schema of functions for the specified language."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "An object for analyzing code metrics, used by the handler."}], "returns": {"type": "Optional[BaseHandler]", "description": "An instance of the corresponding language handler if supported, otherwise `None`."}, "raises": [{"exception": "None", "description": "This function does not raise exceptions but logs errors if the function schema is `None` or if the language is unsupported."}], "description": "This module is part of the `language_functions` package, which provides handlers for extracting code structures, inserting documentation comments, and validating code across various programming languages. The module imports specific handlers for languages like Python, Java, JavaScript/TypeScript, Go, C++, HTML, and CSS. It also imports the `MetricsAnalyzer` for analyzing code metrics. The main function, `get_handler`, is a factory function that retrieves the appropriate handler based on the specified programming language and function schema. It normalizes the language input to ensure case-insensitive matching and returns an instance of the corresponding handler class if supported, or logs an error and returns `None` if the language is unsupported or if the function schema is `None`.", "metadata": {}, "complexity": 5, "validation_status": true, "validation_errors": []}, "timestamp": 1733742660.9468246}, "1546343290d8c9bb705288f54aed9624e47b0e1e5577dbcbd7b49a52b11e29c9": {"value": {"summary": "Manages code chunking operations for different programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a chunk. Default is 4096."}, {"name": "overlap", "type": "int", "description": "The number of tokens by which chunks can overlap. Default is 200."}], "returns": {"type": "ChunkManager", "description": "An instance of the `ChunkManager` class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any other errors encountered during chunk creation."}], "description": "The `ChunkManager` class is responsible for managing the operations related to code chunking. It provides functionality to create, split, and merge code chunks. The class supports context-aware chunking for Python code using Abstract Syntax Tree (AST) analysis and simple line-based chunking for other languages. It utilizes a `TokenManager` to count tokens and ensure that chunks do not exceed a specified token limit. This is particularly useful for processing large codebases where operations need to be performed on manageable pieces of code.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733743138.6028073}, "8483753621b15af2a0c853beef66162915964afc5ac1df0290b1d039800abf59": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code, represented as strings."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the code being analyzed."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code and identify dependencies by examining import statements and usage of imported names. It leverages the Abstract Syntax Tree (AST) module to parse the code and traverse the nodes to find all imported modules and functions that are used within the code. This tool is particularly useful for understanding code dependencies, refactoring, or preparing for module packaging.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733743142.7297373}, "dd21fa7cacff7e12295662e8ab0a23c3c41fd34590ce9bd2514153a378d231ce": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "Flag indicating whether to use the cache for reading the file."}, {"name": "encoding", "type": "str", "description": "The encoding to use for reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised when the file cannot be decoded with the specified encoding."}, {"exception": "Exception", "description": "General exception raised for errors during file reading."}], "description": "The `FileHandler` class provides methods for asynchronous file operations with support for caching and error handling. It is designed to efficiently read file contents while minimizing disk access through caching. The class manages a cache of file contents, allowing repeated reads of the same file to be served from memory, thus improving performance. The class also handles common file reading errors, such as `UnicodeDecodeError`, by attempting to read the file with error handling strategies.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733743162.3059113}, "020be88f0dcc6699379164cc2e2d0ecfe74ebdef91c0c4da827d2c0d8622637b": {"value": {"summary": "Exception class for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [], "description": "The `MetricsCalculationError` class is a custom exception that serves as the base exception for errors encountered during metrics calculations. It inherits from the built-in `Exception` class and does not add any additional functionality or attributes.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743164.1192555}, "ce9f9d999c9744ad1e96a5569092227713328529fe5b9e2db3a86cd77fdc1597": {"value": {"summary": "Exception raised for missing code chunks.", "args": [], "returns": {"type": "None", "description": "This exception does not return any value."}, "raises": [], "description": "The `ChunkNotFoundError` class is an exception that is raised when a requested code chunk cannot be found in the context tree. This exception is typically used in scenarios where a code chunk is expected to be present, but is not found, indicating potential issues with chunk management or retrieval.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743166.6797693}, "57135f43ce87905ccce37cce2b609bc09b893b1e23d911c33786533af0536128": {"value": {"summary": "Manages code chunking operations for various programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between consecutive chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any other errors encountered during chunking operations."}], "description": "The `ChunkManager` class is responsible for managing code chunking operations. It provides methods to create, split, and merge code chunks, with a focus on handling Python code using Abstract Syntax Tree (AST) analysis. The class supports chunking operations for different programming languages by considering token limits and overlaps. It utilizes a `TokenManager` to handle token counting and ensure chunks adhere to specified limits.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733743240.2413635}, "56989c2beaa42d994ec88d9f879197c45022eaa533b341a88801568656063cca": {"value": {"summary": "Analyzes dependencies within a Python file using the Abstract Syntax Tree (AST).", "args": [{"name": "code", "type": "str", "description": "The source code to analyze for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of strings representing the dependencies found in the provided source code."}, "raises": [{"exception": "SyntaxError", "description": "Raised if the provided source code contains syntax errors that prevent parsing."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code to identify dependencies by examining import statements and usage of imported names. It leverages the Abstract Syntax Tree (AST) to parse and traverse the code, collecting information about imported modules and objects, and determining which are actually used as dependencies within the code. This class can be particularly useful for static code analysis, dependency management, and understanding module interdependencies in a Python project.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733743243.6888514}, "933dfaa3bec404fd3f0c6982dbab38f81b5a8324baec69adade77372157d40a3": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file that needs to be read."}, {"name": "use_cache", "type": "bool", "description": "Flag indicating whether to use cached content if available."}, {"name": "encoding", "type": "str", "description": "The encoding to use when reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised if the file cannot be decoded using the specified encoding."}, {"exception": "Exception", "description": "Raised for any other errors encountered during file reading."}], "description": "The `FileHandler` class provides asynchronous methods for reading file contents with support for caching. It is designed to handle common file reading errors, such as encoding issues, and offers a mechanism to cache file contents to improve performance. The class uses a thread-safe cache and a thread pool executor to manage file reading tasks efficiently.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733743250.5512474}, "11e65f3bf3163a58359bbd3776ee49f634bcdf2dc708a4dc630bfd082e590d24": {"value": {"summary": "Exception for errors in metrics calculation.", "args": [], "returns": {"type": "None", "description": "This exception does not return any value."}, "raises": [{"exception": "MetricsCalculationError", "description": "Raised when there is an error in metrics calculation."}], "description": "MetricsCalculationError is a custom exception class used to signal errors that occur during the calculation of metrics. It serves as a base exception for all metric-related errors, allowing for more specific error handling in the application.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743252.6610577}, "ca823da6eff9f37e4b15fcc6393d63b6e12121df57fa1d8710cb24756686d5f5": {"value": {"summary": "Raised when a requested chunk cannot be found.", "args": [], "returns": {"type": "None", "description": "This exception does not return a value as it is used for signaling error conditions."}, "raises": [], "description": "This exception is raised when an attempt is made to access a code chunk that does not exist in the current context. It is a custom exception that inherits from the base Exception class and provides a specific error message indicating the absence of the requested chunk.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743259.4476082}, "d2bf88cb773920413dddc15a9d4a3944003af7af2ba31f269ee62ec23f9bd9b2": {"value": {"summary": "A module for integrating with OpenAI's GPT-4o model.", "args": [{"name": "api_key", "type": "str", "description": "The API key for authenticating with the OpenAI API."}, {"name": "base_info", "type": "str", "description": "Basic information to be included in the prompt as a system message."}, {"name": "context", "type": "str", "description": "Contextual information to be included in the prompt as a user message."}, {"name": "chunk_content", "type": "str", "description": "Content to be included in the prompt as an assistant message."}, {"name": "schema", "type": "Dict[str, Any]", "description": "A schema dictionary to be included in the prompt."}, {"name": "prompt", "type": "List[Dict[str, str]]", "description": "A list of dictionaries representing the structured prompt for the OpenAI model."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens to be used in the generated documentation."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class."}, "raises": [{"exception": "openai.error.OpenAIError", "description": "Raised when there is an issue with the OpenAI API call."}, {"exception": "ValueError", "description": "Raised if the prompt is improperly formatted or contains invalid data."}], "description": "The openai_model module provides a class, OpenAIModel, that facilitates interaction with OpenAI's GPT-4o model. It allows users to generate structured prompts, calculate token usage, and fetch generated documentation from the model.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743265.830681}, "79459d241f88b0136b61274b83bc35b2cc34a58bace6633f5346595fcc99f7c1": {"value": {"summary": "Provides classes and functions for generating and writing documentation reports.", "args": [], "returns": {"type": "None", "description": "This module does not return any value directly."}, "raises": [], "description": "This module is responsible for generating and writing documentation reports in JSON and Markdown formats. It includes classes for handling errors related to documentation, badge configuration and generation, Markdown formatting, and comprehensive documentation generation using templates. The module also provides an asynchronous function to write documentation reports to specified file paths, ensuring thread safety with a global write lock.", "metadata": {}, "complexity": 0, "validation_status": true, "validation_errors": []}, "timestamp": 1733743268.2078278}, "82d94af62fe7937a93a8d68aa1481660639f7869cb10dd7c86776704baa3b1f4": {"value": {"summary": "Handles interaction with the Gemini model API.", "args": [{"name": "api_key", "type": "str", "description": "API key for authenticating requests to the Gemini model API."}, {"name": "endpoint", "type": "str", "description": "The base URL endpoint for the Gemini model API."}], "returns": {"type": "None", "description": "This class does not return a value upon initialization."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised if there is an issue with the HTTP request to the Gemini API."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error in decoding the JSON response from the Gemini API."}], "description": "The GeminiModel class provides methods to interact with the Gemini model API for generating documentation and calculating token counts. It includes methods to generate documentation prompts, calculate token usage, and structure prompts for API requests. The class uses asynchronous HTTP requests to communicate with the Gemini API and handles JSON payloads for documentation generation.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733743272.6491566}, "9329344fef987caa6a8d50bfedca5c8d81e9d3f0ec8b399f0beb895a5702e9c9": {"value": {"summary": "API request model for documentation generation.", "args": [{"name": "file_paths", "type": "List[str]", "description": "A list of file paths to be processed for documentation generation."}, {"name": "skip_types", "type": "Optional[List[str]]", "description": "A list of file types to skip during processing. Defaults to an empty list."}, {"name": "project_info", "type": "Optional[str]", "description": "Additional information about the project. Defaults to an empty string."}, {"name": "style_guidelines", "type": "Optional[str]", "description": "Style guidelines to be followed during documentation generation. Defaults to an empty string."}, {"name": "safe_mode", "type": "Optional[bool]", "description": "Flag to indicate if the process should run in safe mode. Defaults to False."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project."}, {"name": "provider", "type": "str", "description": "The AI provider to use for documentation generation. Defaults to 'azure'."}, {"name": "max_concurrency", "type": "Optional[int]", "description": "Maximum number of concurrent tasks allowed. Defaults to 5."}, {"name": "priority", "type": "Optional[str]", "description": "Priority level of the task. Can be 'low', 'normal', or 'high'. Defaults to 'normal'."}, {"name": "callback_url", "type": "Optional[str]", "description": "URL to be called upon task completion. Defaults to None."}], "returns": {"type": "DocumentationRequest", "description": "An instance of DocumentationRequest with validated attributes."}, "raises": [{"exception": "ValueError", "description": "Raised if the provider is not one of the valid options ('azure', 'gemini', 'openai')."}, {"exception": "ValueError", "description": "Raised if the priority is not one of the valid options ('low', 'normal', 'high')."}], "description": "The DocumentationRequest class is a Pydantic model that defines the structure of the API request for generating documentation. It includes various attributes to specify the files to be processed, the provider to be used, concurrency settings, and more. Validators are provided to ensure that the provider and priority values are valid.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733743281.611315}, "6b6415f61395a24b686a3aa40d5e416aa42eb6576c968c285ed8718207a4ba6e": {"value": {"summary": "Enumeration of supported tokenizer models.", "args": [], "returns": {"type": "TokenizerModel", "description": "An enumeration representing the supported tokenizer models."}, "raises": [], "description": "The `TokenizerModel` class is an enumeration that defines the supported tokenizer models used within the tokenization module. Each member of this enumeration represents a specific tokenizer model that can be utilized for encoding and decoding operations. The models included are GPT4, GPT3, and CODEX, each associated with a unique encoding base.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743288.715751}, "bcbcd91f385622b63bfda622ba5f94d8b22ca01b8b0f5124cf9e637139525556": {"value": {"summary": "Manage and render templates using Jinja2.", "args": [{"name": "template_dir", "type": "str", "description": "Directory containing the templates."}, {"name": "template_name", "type": "str", "description": "Name of the template to retrieve or render."}, {"name": "context", "type": "dict", "description": "Dictionary of variables to pass to the template for rendering."}, {"name": "name", "type": "str", "description": "Name to reference the custom template."}, {"name": "template_path", "type": "str", "description": "Path to the custom template file."}], "returns": {"type": "Template | str | None", "description": "The requested template, rendered template string, or None for adding a custom template."}, "raises": [{"exception": "ValueError", "description": "Raised when the template name is invalid."}, {"exception": "FileNotFoundError", "description": "Raised when the custom template file is not found."}, {"exception": "Exception", "description": "Raised for general exceptions during template loading and rendering."}], "description": "The TemplateManager module provides functionality to manage and render templates using the Jinja2 templating engine. It allows for the retrieval, rendering, and addition of custom templates, utilizing a caching mechanism to optimize performance. The module is designed to work asynchronously, making it suitable for use in asynchronous applications.", "metadata": {}, "complexity": 9, "validation_status": true, "validation_errors": []}, "timestamp": 1733743292.0099268}, "5d084297a956a7fa5fc8dd1241aa18c56ce2ed3e5677592986ff8ca802ba0c67": {"value": {"summary": "Configuration model for AI providers.", "args": [{"name": "name", "type": "str", "description": "The name of the AI provider."}, {"name": "endpoint", "type": "str", "description": "The API endpoint for the AI provider."}, {"name": "api_key", "type": "str", "description": "The API key used for authenticating with the AI provider."}, {"name": "deployment_name", "type": "Optional[str]", "description": "The name of the deployment, if applicable."}, {"name": "api_version", "type": "Optional[str]", "description": "The version of the API to use."}, {"name": "model_name", "type": "Optional[str]", "description": "The name of the AI model to use."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens to use in a request. Defaults to 4096."}, {"name": "temperature", "type": "float", "description": "The temperature setting for the AI model, controlling randomness. Must be between 0 and 1. Defaults to 0.7."}, {"name": "max_retries", "type": "int", "description": "The maximum number of retries for failed requests. Defaults to 3."}, {"name": "retry_delay", "type": "float", "description": "The delay between retries in seconds. Defaults to 1.0."}, {"name": "cache_enabled", "type": "bool", "description": "Flag to enable or disable caching. Defaults to True."}, {"name": "timeout", "type": "float", "description": "The timeout for requests in seconds. Defaults to 30.0."}, {"name": "chunk_overlap", "type": "int", "description": "The overlap size for chunks in requests. Defaults to 200."}, {"name": "min_chunk_size", "type": "int", "description": "The minimum size for chunks in requests. Defaults to 100."}, {"name": "max_parallel_chunks", "type": "int", "description": "The maximum number of parallel chunks allowed. Defaults to 3."}], "returns": {"type": "ProviderConfig", "description": "An instance of the ProviderConfig class with validated attributes."}, "raises": [{"exception": "ValueError", "description": "Raised if the `temperature` is not between 0 and 1."}, {"exception": "ValueError", "description": "Raised if `max_tokens` is not between 1 and 8192."}], "description": "The `ProviderConfig` class is a configuration model that stores and validates settings for AI providers. It inherits from Pydantic's `BaseModel`, enabling data validation and parsing. The class includes attributes such as `name`, `endpoint`, `api_key`, and others, which are essential for configuring AI provider interactions. It also includes validators to ensure that certain parameters, like `temperature` and `max_tokens`, fall within acceptable ranges.", "metadata": {}, "complexity": 4, "validation_status": true, "validation_errors": []}, "timestamp": 1733743301.7268972}, "86934bb538bbce78582fa08cc41b7a14017f496c3976a9ee2cfc8555f9ae0ab4": {"value": {"summary": "Calculates total tokens for the prompt content using TokenManager.", "args": [{"name": "base_info", "type": "str", "description": "Project and style information."}, {"name": "context", "type": "str", "description": "Related code or documentation context."}, {"name": "chunk_content", "type": "str", "description": "Content of the chunk being documented."}, {"name": "schema", "type": "str", "description": "JSON schema representing the structure."}], "returns": {"type": "int", "description": "The total count of tokens across all provided text components."}, "raises": [{"exception": "TokenManagerError", "description": "Raised if there is an error in counting tokens using TokenManager."}], "description": "This function calculates the total number of tokens present in the provided prompt content. It utilizes the TokenManager to count tokens for each text component, including base information, context, chunk content, and schema. The total token count is accumulated and returned as the result.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733743305.3439023}, "c242f3a5c3b32ce493ef82bd47c1bc187f189a6cb7d53002b7cde47a6b5321d0": {"value": {"summary": "Handles API interactions with error handling and rate limiting.", "args": [{"name": "session", "type": "aiohttp.ClientSession", "description": "An asynchronous HTTP client session for making API requests."}, {"name": "config", "type": "ProviderConfig", "description": "Configuration settings for the API provider, including endpoints, API keys, and retry settings."}, {"name": "semaphore", "type": "asyncio.Semaphore", "description": "A semaphore to limit the number of concurrent API requests."}, {"name": "metrics_manager", "type": "MetricsManager", "description": "A manager for recording and tracking API call metrics, such as latency and token usage."}], "returns": {"type": "None", "description": "This class does not return a value. It manages API interactions internally."}, "raises": [{"exception": "Exception", "description": "Raised when an error occurs during API interaction, such as network issues or invalid responses."}, {"exception": "ValueError", "description": "Raised when an unsupported provider is specified."}], "description": "The `APIHandler` class is responsible for managing interactions with various API providers. It includes mechanisms for error handling, retry logic, and rate limiting to ensure efficient and reliable communication with APIs. The class supports multiple providers, including Azure, Gemini, and OpenAI, and can handle different types of API responses. It also tracks API call metrics and manages rate limit tokens to prevent exceeding provider limits.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733743311.251859}, "f0b019b988b0ee20449238310729c1e32edb808cb54e25d463b60b05f8aedd66": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens allowed to overlap between chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the code being chunked."}, {"exception": "Exception", "description": "Raised for any general errors that occur during the chunking process."}], "description": "The `ChunkManager` class is responsible for managing the operations related to code chunking. It provides methods to create, split, and merge code chunks, ensuring that each chunk adheres to specified token limits. This is particularly useful for processing large code files in a way that preserves context and functionality. The class supports different programming languages, with specialized handling for Python code using Abstract Syntax Tree (AST) analysis.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733743321.6114955}, "edc23825438625a7bc3707785a0ea5a68f731de17ecf87221c79a177fa1259ef": {"value": {"summary": "Analyzes dependencies in Python code using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to be analyzed for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of strings representing the dependencies found within the provided source code."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the provided source code, preventing successful parsing."}], "description": "The DependencyAnalyzer class is designed to analyze dependencies within a given Python source code by leveraging the Abstract Syntax Tree (AST) module. It identifies and records dependencies by visiting various AST nodes related to imports and function calls. The class extends ast.NodeVisitor to traverse the AST and extract information about imported modules and their usage within the code. This tool is particularly useful for understanding code dependencies, refactoring, or preparing for code migration.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733743323.9011364}, "9ceeba66d58bf95589e20919161b476b9c1ec5fefd761f1b9a6a2440e3ebd0b7": {"value": {"summary": "Manages code chunking operations for various programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens that can overlap between consecutive chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the provided Python code."}, {"exception": "Exception", "description": "Raised for any other errors encountered during chunk creation."}], "description": "The `ChunkManager` class is responsible for managing the process of dividing code into manageable chunks, particularly for Python and other languages. It uses tokenization to ensure that the chunks do not exceed a specified token limit, while also allowing for a certain overlap between chunks. The class provides methods to create, split, and merge code chunks, utilizing abstract syntax tree (AST) analysis for Python code to maintain context awareness. It also handles errors gracefully, logging them for further analysis.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733743481.5723264}, "c0fa34d3223f55c5bd464712d59dcb14d66b53ec1c18e654570b283910e65ee1": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze. This should be a string representation of Python code."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code. Each dependency is represented as a string, indicating the name of the module or function that is imported and used."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the provided code. The error is logged, and an empty set of dependencies is returned."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code to determine its dependencies. It utilizes the Abstract Syntax Tree (AST) module to parse the code and visit various nodes representing import statements and function calls. By visiting these nodes, the class identifies which modules and functions are imported and used within the code, thereby determining the dependencies. This can be particularly useful for understanding the external libraries and modules a piece of Python code relies on, aiding in tasks such as code refactoring, dependency management, and security analysis.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733743489.7925477}, "7d301c5e0eb9e35de71e6796b02c76d37dbdfb4d65cb592d264ea625002fb810": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "Flag to determine if the file content should be cached for future reads."}, {"name": "encoding", "type": "str", "description": "The encoding to use when reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised when the file cannot be decoded using the specified encoding."}, {"exception": "Exception", "description": "Raised for any other exceptions that occur during file reading."}], "description": "The `FileHandler` class provides methods for asynchronous file operations, with built-in caching and error handling capabilities. It is designed to efficiently read file contents while minimizing disk I/O through caching. The class uses a thread-safe cache to store file contents, which can be accessed asynchronously. It also handles common file reading errors, such as encoding issues, by attempting alternative reading strategies.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733743495.0440586}, "dc594f8c88149abc21ed1fc0f94dcbb0eb4ee7fc3a266c07a5261681ebe0c9dc": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return a value as it is used for exception handling."}, "raises": [], "description": "The `MetricsCalculationError` class serves as a custom exception for handling errors related to metrics calculation. It extends the base `Exception` class, allowing users to raise specific errors when metrics calculations go wrong.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743496.994553}, "83337f8b43076450d834390d0e733cd3e41ed763ffa2485385081e2e5fd1abac": {"value": {"summary": "Exception raised when a requested chunk is not found.", "args": [], "returns": {"type": "None", "description": "This exception does not return a value."}, "raises": [], "description": "The `ChunkNotFoundError` is an exception that is raised when a specific code chunk requested by the user cannot be located within the context tree. This exception helps in identifying issues related to missing code chunks during operations such as retrieval or update.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743500.5773823}, "edc7b5c3ebf8fe1066c2de5973256bb729f89f40fecbdde4253ab3601483f0e3": {"value": {"summary": "A module for integrating with OpenAI's GPT-4o model to generate prompts and documentation.", "args": [{"name": "api_key", "type": "str", "description": "The API key used for authenticating requests to the OpenAI API."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class, ready to interact with the OpenAI API."}, "raises": [{"exception": "openai.error.OpenAIError", "description": "Raised if there is an issue with the OpenAI API request."}, {"exception": "ValueError", "description": "Raised if the provided API key is invalid."}], "description": "The `openai_model` module provides a class `OpenAIModel` that facilitates interaction with OpenAI's GPT-4o model. It allows users to generate structured prompts, calculate token usage, and fetch AI-generated documentation. This module is particularly useful for applications that require dynamic content generation and interaction with OpenAI's language models.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743504.367612}, "239cc4d7df1ff59012cd47e268ea23f4caa2ed0b710ea355f0b785140eb5fa2f": {"value": {"summary": "Base exception for documentation-related errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [], "description": "The `DocumentationError` class serves as the base exception for all errors related to documentation processing within the module. It is used to signal issues that arise during documentation generation, formatting, or writing.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743507.296534}, "82cad7b40f049296d5f49f351c2a63abbf2213d896cee614495e9eeafe54c741": {"value": {"summary": "Handles interactions with the Gemini model API for documentation generation and token calculation.", "args": [{"name": "api_key", "type": "str", "description": "The API key used for authenticating requests to the Gemini model API."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL for the Gemini model API."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised when there is an issue with the HTTP request to the Gemini API."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error decoding the JSON response from the API."}], "description": "The `GeminiModel` class provides methods to interact with the Gemini model API. It facilitates the generation of documentation through the API and calculates token counts for prompts. The class utilizes asynchronous HTTP requests to communicate with the API and employs a token manager for token counting. This class is essential for integrating with Gemini's capabilities in generating code documentation and managing token usage efficiently.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733743514.5454519}, "37f21cb226e2204c23af74429abf97c07ef6cd0f7ea700f74b6e4604f5febd24": {"value": {"summary": "API request model for documentation generation.", "args": [{"name": "file_paths", "type": "List[str]", "description": "A list of file paths to be processed for documentation generation."}, {"name": "skip_types", "type": "Optional[List[str]]", "description": "A list of file types to be skipped during processing."}, {"name": "project_info", "type": "Optional[str]", "description": "Additional information about the project."}, {"name": "style_guidelines", "type": "Optional[str]", "description": "Guidelines for the style of the generated documentation."}, {"name": "safe_mode", "type": "Optional[bool]", "description": "Flag to enable or disable safe mode during processing."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project."}, {"name": "provider", "type": "str", "description": "The AI provider to use for documentation generation. Must be one of 'azure', 'gemini', or 'openai'."}, {"name": "max_concurrency", "type": "Optional[int]", "description": "Maximum number of concurrent tasks allowed during processing."}, {"name": "priority", "type": "Optional[str]", "description": "Priority level of the request, which can be 'low', 'normal', or 'high'."}, {"name": "callback_url", "type": "Optional[str]", "description": "URL to be called upon completion of the task."}], "returns": {"type": "DocumentationRequest", "description": "An instance of DocumentationRequest initialized with the provided parameters."}, "raises": [{"exception": "ValueError", "description": "Raised if the provider or priority values are not within the allowed options."}], "description": "The DocumentationRequest class serves as a model for API requests related to documentation generation. It encapsulates various parameters necessary for the request, including file paths, project information, and provider details. The class also includes validation methods for certain attributes to ensure they meet predefined constraints.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733743519.432264}, "0b45b9c3c1c6f42354e5f79ac0f3b2b5b7ca4624444e5c89cf87209a03ba3e25": {"value": {"summary": "Defines supported tokenizer models for text processing.", "args": [], "returns": {"type": "None", "description": "This class does not return a value as it is an enumeration of constants."}, "raises": [], "description": "The `TokenizerModel` class is an enumeration that specifies the different tokenizer models supported by the tokenization system. These models are used to encode and decode text data, facilitating various natural language processing tasks. Each model corresponds to a specific encoding strategy, tailored for different versions of the GPT models and Codex.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743521.4324358}, "47d6dfd58aaa2b5b030d1f21c981136a901ca45821144400636b3844b53ef52c": {"value": {"summary": "Manage and render templates using Jinja2.", "args": [{"name": "template_dir", "type": "str", "description": "Directory containing the templates."}, {"name": "template_name", "type": "str", "description": "Name of the template to retrieve or render."}, {"name": "context", "type": "dict", "description": "Dictionary of variables to pass to the template for rendering."}, {"name": "name", "type": "str", "description": "Name to reference the custom template."}, {"name": "template_path", "type": "str", "description": "Path to the custom template file."}], "returns": {"type": "Template", "description": "The requested template object."}, "raises": [{"exception": "ValueError", "description": "Raised if the template name is invalid."}, {"exception": "FileNotFoundError", "description": "Raised if the custom template file is not found."}, {"exception": "Exception", "description": "Raised if there is an error loading or rendering the template."}], "description": "The `TemplateManager` class provides functionality to manage and render templates using the Jinja2 templating engine. It allows users to retrieve templates by name, render them with specific context data, and add custom templates to the manager. The class utilizes caching to optimize template retrieval and rendering operations, reducing the need to repeatedly load templates from the file system. This module is particularly useful for applications that require dynamic content generation, such as documentation tools or web applications.", "metadata": {}, "complexity": 9, "validation_status": true, "validation_errors": []}, "timestamp": 1733743525.7154958}, "8710f51516179ba72d30ae8105561f555654f4b58b54e61442ddc10ccfc99f9a": {"value": {"summary": "Configuration model for AI providers.", "args": [{"name": "name", "type": "str", "description": "The name of the AI provider."}, {"name": "endpoint", "type": "str", "description": "The API endpoint for the AI provider."}, {"name": "api_key", "type": "str", "description": "The API key used for authenticating requests to the provider."}, {"name": "deployment_name", "type": "Optional[str]", "description": "The name of the deployment, if applicable."}, {"name": "api_version", "type": "Optional[str]", "description": "The version of the API to use."}, {"name": "model_name", "type": "Optional[str]", "description": "The name of the model to use."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens to generate in a single request. Defaults to 4096."}, {"name": "temperature", "type": "float", "description": "The sampling temperature to use, which affects randomness. Must be between 0 and 1. Defaults to 0.7."}, {"name": "max_retries", "type": "int", "description": "The maximum number of times to retry a failed request. Defaults to 3."}, {"name": "retry_delay", "type": "float", "description": "The delay in seconds between retry attempts. Defaults to 1.0."}, {"name": "cache_enabled", "type": "bool", "description": "Whether caching is enabled for requests. Defaults to True."}, {"name": "timeout", "type": "float", "description": "The timeout in seconds for API requests. Defaults to 30.0."}, {"name": "chunk_overlap", "type": "int", "description": "The number of tokens to overlap between chunks. Defaults to 200."}, {"name": "min_chunk_size", "type": "int", "description": "The minimum size of chunks in tokens. Defaults to 100."}, {"name": "max_parallel_chunks", "type": "int", "description": "The maximum number of chunks to process in parallel. Defaults to 3."}], "returns": {"type": "ProviderConfig", "description": "An instance of the ProviderConfig class with the specified configuration."}, "raises": [{"exception": "ValueError", "description": "Raised if the `temperature` is not between 0 and 1 or if `max_tokens` is not between 1 and 8192."}], "description": "The `ProviderConfig` class is a configuration model for AI providers, extending the `BaseModel` from Pydantic. It is designed to hold various configuration parameters that are essential for connecting and interacting with AI provider services. This includes details such as API endpoints, authentication keys, model specifications, and operational parameters like timeout and retries. The class also includes validation logic to ensure that certain attributes, such as `temperature` and `max_tokens`, fall within acceptable ranges.", "metadata": {}, "complexity": 4, "validation_status": true, "validation_errors": []}, "timestamp": 1733743549.5580044}, "fa72b9f0e516e8fb5ea9c428c999afc94f34d89632f2ec5f0e851c742dff21a8": {"value": {"summary": "Calculates total tokens for the prompt content using TokenManager.", "args": [{"name": "base_info", "type": "str", "description": "Project and style information that contributes to the prompt."}, {"name": "context", "type": "str", "description": "Related code or documentation that provides context for the prompt."}, {"name": "chunk_content", "type": "str", "description": "The content of the chunk being documented, which is part of the prompt."}, {"name": "schema", "type": "str", "description": "JSON schema that defines the structure of the prompt."}], "returns": {"type": "int", "description": "The total token count for the prompt content."}, "raises": [{"exception": "TokenManagerError", "description": "If there is an error in counting tokens using TokenManager."}], "description": "This function computes the total number of tokens required for a given prompt content by leveraging the TokenManager utility. It takes into account various components of the prompt such as base information, context, chunk content, and the schema. Each component's token count is calculated and summed up to provide the total token count for the prompt.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733743553.4254656}, "da8cd57b4e0a1006f3e7b0720c44e0108fa7b97f1b86c6bf8d32be52ae1a66c9": {"value": {"summary": "Handles API interactions with error handling and rate limiting.", "args": [{"name": "session", "type": "aiohttp.ClientSession", "description": "An asynchronous HTTP client session used to make API requests."}, {"name": "config", "type": "ProviderConfig", "description": "Configuration settings for the API provider, including rate limits and retry policies."}, {"name": "semaphore", "type": "asyncio.Semaphore", "description": "A semaphore to limit the number of concurrent API requests."}, {"name": "metrics_manager", "type": "MetricsManager", "description": "An object to record metrics related to API calls, such as success rates and latencies."}], "returns": {"type": "ProcessingResult", "description": "The result of the API call, including success status, content, processing time, and error information if applicable."}, "raises": [{"exception": "Exception", "description": "Raised when an error occurs during API interaction, such as network issues or invalid responses."}], "description": "The `APIHandler` class is responsible for managing interactions with external APIs, including handling errors and implementing rate limiting. It provides methods to fetch completions from different AI service providers such as Azure, Gemini, and OpenAI. The class ensures that API requests are made within the allowed rate limits and retries requests in case of transient errors. It also records metrics related to API calls, such as latency and token usage.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733743558.8277116}, "ab7e4afeab4c9e1883c22f7e9706040835152884ce960d9d62db28a8aba0f8e3": {"value": {"summary": "Main module for generating and inserting docstrings.", "args": [{"name": "repo_path", "type": "str", "description": "Path to the code repository where docstrings will be generated and inserted."}, {"name": "config", "type": "str", "description": "Path to the configuration file (config.json) containing settings for the documentation generation process."}, {"name": "provider", "type": "str", "description": "AI provider to use for generating docstrings. Options are 'azure', 'gemini', or 'openai'."}, {"name": "concurrency", "type": "int", "description": "Number of concurrent requests to be made during the documentation generation process."}, {"name": "skip_types", "type": "str", "description": "Comma-separated list of file extensions to skip during the documentation generation process."}, {"name": "project_info", "type": "str", "description": "Additional information about the project to be included in the documentation."}, {"name": "style_guidelines", "type": "str", "description": "Documentation style guidelines to be followed during docstring generation."}, {"name": "safe_mode", "type": "bool", "description": "Flag to run the process in safe mode, where no files are modified."}, {"name": "log_level", "type": "str", "description": "Logging level for the process. Determines the verbosity of log messages."}, {"name": "schema", "type": "str", "description": "Path to the function schema file (function_schema.json) used for validating and generating docstrings."}, {"name": "doc_output_dir", "type": "str", "description": "Directory where the generated documentation files will be saved."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project, required for processing."}], "returns": {"type": "None", "description": "This module does not return a value. It performs operations and logs the results."}, "raises": [{"exception": "SystemExit", "description": "Raised when the process needs to exit due to an error, such as unsupported provider or failure to set up logging."}, {"exception": "Exception", "description": "Catches any unhandled exceptions during the execution of the main function."}, {"exception": "KeyboardInterrupt", "description": "Raised when the process is interrupted by the user."}], "description": "This module provides a command-line interface to generate and insert docstrings into a code repository using AI models from Azure OpenAI, Gemini, or OpenAI. It parses command-line arguments to configure the process, loads necessary configurations, initializes the appropriate AI model client, and manages the documentation generation process. The module supports concurrent processing, logging, and safe mode operations.", "metadata": {}, "complexity": 8, "validation_status": true, "validation_errors": []}, "timestamp": 1733743572.1154673}, "3aa74dfba353db98f9854a2a06f51253ac215f2ed3d8be3c041d2e1692f30a4c": {"value": {"summary": "Handles interaction with the Azure OpenAI API for documentation generation.", "args": [{"name": "api_key", "type": "str", "description": "The API key for authenticating requests to the Azure OpenAI API."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL for the Azure OpenAI API."}, {"name": "deployment_name", "type": "str", "description": "The name of the deployment to use for API requests."}, {"name": "api_version", "type": "str", "description": "The version of the API to use for requests."}], "returns": {"type": "AzureModel", "description": "An instance of the AzureModel class."}, "raises": [{"exception": "aiohttp.ClientError", "description": "If there is an issue with the HTTP request to the Azure API."}, {"exception": "json.JSONDecodeError", "description": "If the response from the Azure API cannot be decoded as JSON."}], "description": "The AzureModel class provides methods to interact with the Azure OpenAI API. It facilitates the generation of documentation by sending prompts to the API and processing the responses. Additionally, it includes functionality to calculate token counts for prompts and generate structured prompts for API requests. The class requires API credentials and configuration details to function properly.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733743575.3099153}, "665b74135f39c94a49a41bcb0ef8c322c5745847570a330dac2d1e740c47e7ec": {"value": {"summary": "Handles Python code analysis and manipulation.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "A dictionary representing the schema to validate the code structure against."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "An instance of MetricsAnalyzer to manage and analyze code metrics."}], "returns": {"type": "None", "description": "This constructor does not return any value."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being analyzed."}, {"exception": "ValidationError", "description": "Raised when the extracted code structure does not conform to the provided schema."}, {"exception": "Exception", "description": "Raised for any general exceptions that occur during code analysis."}], "description": "The `PythonHandler` class is responsible for analyzing Python code by extracting its structure, inserting docstrings, and validating the code. It utilizes various libraries such as `ast` for parsing, `jsonschema` for schema validation, and `radon` for complexity analysis. The class provides methods to extract code structure, insert Google-style docstrings, and validate code using pylint. It also manages metrics and schema validation for the extracted code structure.", "metadata": {}, "complexity": 30, "validation_status": true, "validation_errors": []}, "timestamp": 1733743587.6535249}, "46f6fd6881230570c39c409d23e333bbbfb2f7c9d8dd930e1ad0d50ffac64319": {"value": {"summary": "Handles CSS code files, providing structure extraction, documentation insertion, and validation.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "A schema that defines functions for generating documentation."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [], "description": "The `CSSHandler` class is designed to manage CSS code files by extracting their structure, inserting documentation comments, and validating the code. It leverages external JavaScript scripts to parse and modify the code, ensuring accurate processing of CSS elements. The class extends the `BaseHandler` abstract class, inheriting its foundational capabilities. This handler is particularly useful for developers and tools that need to programmatically analyze and modify CSS files.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733743592.282018}, "cddc8d2003a2b7b84f9e40809fe862fb854b4cae02ba0a59c4efe10f2f8db891": {"value": {"summary": "Handler for HTML language code processing.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations, providing necessary configurations and parameters."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [], "description": "The HTMLHandler class is designed to process HTML code files. It provides methods to extract the structure of the HTML code, insert comments based on documentation, and validate the correctness of the HTML code. This class extends the BaseHandler abstract class and utilizes external JavaScript scripts for parsing and modifying HTML code.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733743594.9778192}, "795032ebaffc711d7b5ae8f8358b22a9f11393df34ad40aa53c3431f4006aa9c": {"value": {"summary": "Handler for the Go programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [], "description": "The `GoHandler` class is designed to manage Go language code files. It provides methods to extract code structure, insert comments, and validate Go code. The class leverages external Go scripts to parse and modify the code, ensuring accurate and efficient handling of Go source files. This class extends the `BaseHandler` abstract class, inheriting its foundational functionalities while adding Go-specific capabilities.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733743606.6756508}, "83fb9ed280ca6251481bd656fc7bb632929f1450aba1c86f1914825a98589283": {"value": {"summary": "Handles Java code operations like structure extraction, documentation insertion, and validation.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations within the handler."}, {"name": "code", "type": "str", "description": "The Java source code to process."}, {"name": "file_path", "type": "Optional[str]", "description": "The file path of the Java source code, used for reference and validation."}, {"name": "documentation", "type": "Dict[str, Any]", "description": "Documentation details to be inserted into the Java code as Javadoc comments."}], "returns": {"type": "None", "description": "This class does not return any value directly. It performs operations on Java code and returns results through its methods."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when a subprocess (JavaScript script or javac) fails to execute properly."}, {"exception": "json.JSONDecodeError", "description": "Raised when the output from a subprocess cannot be parsed as JSON."}, {"exception": "FileNotFoundError", "description": "Raised when `javac` is not installed or not found in the system PATH."}, {"exception": "Exception", "description": "Catches any other unexpected errors during execution."}], "description": "The `JavaHandler` class provides functionalities to handle Java code files. It includes methods for extracting the structure of Java code, inserting Javadoc comments based on provided documentation, and validating Java code for syntax correctness using `javac`. The class relies on external JavaScript scripts for parsing and modifying Java code. It extends the `BaseHandler` abstract class, inheriting its interface and possibly other shared functionalities.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733743618.4982526}, "d0ae138f1d2b433e6e274747b8d21ebeb9c297125e8679cc2818543e082b32a9": {"value": {"summary": "Handler for the C++ programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "None", "description": "This class does not return any value upon initialization."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when an external C++ script execution fails."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error parsing JSON output from the C++ parser."}, {"exception": "FileNotFoundError", "description": "Raised when the 'g++' compiler is not found in the system PATH."}, {"exception": "Exception", "description": "Raised for any unexpected errors during processing."}], "description": "The `CppHandler` class is designed to manage C++ code files, offering functionalities such as extracting code structure, inserting documentation, and validating code syntax. It extends the `BaseHandler` abstract class and uses external C++ scripts for parsing and modifying code. The class is initialized with a function schema that defines functions for documentation generation.", "metadata": {}, "complexity": 17, "validation_status": true, "validation_errors": []}, "timestamp": 1733743624.68355}, "a1f58b4764a723a9220ee26563e898ad240c683f998f0b2f1f8d6f402f219a89": {"value": {"summary": "Enumeration for JavaScript/TypeScript documentation styles.", "args": [], "returns": {"type": "JSDocStyle", "description": "An instance of the JSDocStyle enumeration representing a documentation style."}, "raises": [], "description": "The JSDocStyle class is an enumeration that defines two possible styles for documenting JavaScript and TypeScript code: JSDOC and TSDOC. This enumeration is used to specify the type of documentation comments that should be generated or inserted into the code.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733743627.5203717}, "c51e9169030272703e790aa382162d3b57a21015d3cd4532d3b902491e26795c": {"value": {"summary": "Utility functions for handling programming languages in documentation generation.", "args": [], "returns": {"type": "None", "description": "This module does not return any value directly, as it provides utility functions."}, "raises": [], "description": "This module provides utility functions to manage different programming languages within a documentation generation pipeline. It includes functions to retrieve the appropriate language handler based on the programming language and to insert AI-generated docstrings or comments into the source code. The module supports multiple languages, including Python, Java, JavaScript, TypeScript, Go, C++, HTML, and CSS, by utilizing specific handlers for each language.", "metadata": {}, "complexity": 5, "validation_status": true, "validation_errors": []}, "timestamp": 1733743629.9580963}, "9e17f4d6bbbbd8aa87da52ba6ee761d22b6d7dd38f21acdc772914c0308e7cd4": {"value": {"summary": "Factory function to retrieve the appropriate language handler.", "args": [{"name": "language", "type": "str", "description": "The programming language of the source code. This should be a string representing the language name, such as 'python', 'java', etc."}, {"name": "function_schema", "type": "Dict[str, Any]", "description": "A dictionary defining the functions schema. This schema is used by the handler to understand the structure and requirements of the code."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "An instance of the MetricsAnalyzer class, used to analyze code metrics and provide insights into the code's structure and quality."}], "returns": {"type": "Optional[BaseHandler]", "description": "An instance of the corresponding language handler if the language is supported and the function schema is valid. Returns None if the language is unsupported or the function schema is None."}, "raises": [{"exception": "None", "description": "This function does not raise exceptions. It logs errors and returns None in case of failure."}], "description": "The `get_handler` function is designed to return the appropriate handler for a specified programming language. It utilizes a mapping of language names to handler classes to dynamically instantiate and return the correct handler. This function supports a variety of programming languages, including Python, Java, JavaScript, TypeScript, Go, C++, HTML, and CSS. The function requires a language name, a function schema, and a metrics analyzer to operate correctly. If the language is unsupported or the function schema is not provided, it will return None.", "metadata": {}, "complexity": 7, "validation_status": true, "validation_errors": []}, "timestamp": 1733743635.1158543}, "54aa47dfafea491a14ffc182fc1ec0b7fbd186a91e1c6ab006fd59b1794d6a7b": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed per chunk."}, {"name": "overlap", "type": "int", "description": "The number of overlapping tokens allowed between chunks."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any other errors encountered during the chunking process."}], "description": "The `ChunkManager` class is responsible for managing the operations related to code chunking. It provides functionality to create, split, and merge code chunks, particularly focusing on handling Python code using Abstract Syntax Tree (AST) analysis. The class is designed to handle large code files by breaking them into manageable chunks, ensuring that each chunk does not exceed a specified token limit. This is particularly useful for processing code in environments with token constraints, such as LLMs (Large Language Models).", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733744383.2857592}, "b9a1e800dd9900d36500ace210ca8b93878d679547b4e76955ac627431381e28": {"value": {"summary": "Manages code chunking operations.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between chunks to maintain context."}], "returns": {"type": "ChunkManager", "description": "An instance of the `ChunkManager` class."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any other errors encountered during chunk creation."}], "description": "The `ChunkManager` class provides functionality for managing and manipulating code chunks. It supports creating chunks from code, splitting chunks at specified points, and merging chunks. The class is designed to handle different programming languages, with special handling for Python code using Abstract Syntax Tree (AST) analysis. The class relies on a `TokenManager` to manage token counts and ensure chunks do not exceed specified token limits.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733744556.1792297}, "2833a9a5d0deccc0b1e07333aceb2af995b63e760df8c887f276a6bd99ee5490": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code, represented as strings."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the provided source code during parsing."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code to identify dependencies by utilizing the Abstract Syntax Tree (AST) module. It traverses the AST nodes to detect import statements and function calls, recording the modules and functions that are used within the code. This tool is particularly useful for understanding the external dependencies of a Python script, which can aid in tasks such as dependency management, code refactoring, or static analysis.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733744559.2165077}, "692e9b84fcdd9a4029a247f76561eba142a0c41af242eb80a87fc861bc43ea3f": {"value": {"summary": "Manages code chunking operations for different programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed per chunk. Default is 4096."}, {"name": "overlap", "type": "int", "description": "The number of overlapping tokens allowed between consecutive chunks. Default is 200."}], "returns": {"type": "None", "description": "This class does not return a value. It provides methods for chunk management."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for general errors during the chunking process."}, {"exception": "ValueError", "description": "Raised when invalid parameters are provided, such as an invalid split point or when chunks cannot be merged."}], "description": "The `ChunkManager` class is responsible for managing code chunking operations, specifically for splitting and merging code into manageable chunks based on token count. It supports Python code chunking using Abstract Syntax Tree (AST) analysis and simple line-based chunking for other languages. The class utilizes a `TokenManager` to count tokens and ensure chunks do not exceed a specified maximum token count, with an optional overlap. It provides functionality to create, split, and merge code chunks, handling language-specific nuances and ensuring efficient code management.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733744945.2382042}, "06e548ea566790be7e6a6ebf58ddc5e4bd9429b809bad24f7f8c43fe27ef8a82": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code, represented as strings of module or function names."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the provided source code, preventing it from being parsed."}], "description": "The DependencyAnalyzer class utilizes the Abstract Syntax Tree (AST) module to analyze and identify dependencies in a given Python source code. It traverses the AST nodes to detect import statements and function calls, recording the modules and functions that are imported and used within the code. This analysis helps in understanding the external dependencies of the code, which can be useful for code maintenance, refactoring, or packaging.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733744949.199472}, "524eeb2d97c20fe6d2f619ca6c97a6ca4820c14ed04c5408f63e4602b23f4715": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "Flag indicating whether to use caching for the file content."}, {"name": "encoding", "type": "str", "description": "The encoding to use when reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised if there is an error decoding the file with the specified encoding."}, {"exception": "Exception", "description": "General exception for any other errors that occur during file reading."}], "description": "The `FileHandler` class provides functionality for asynchronous file operations with support for caching and error handling. It is designed to efficiently read file contents while managing a cache to minimize redundant file reads. The class supports reading files with specified encoding and handles potential decoding errors gracefully by attempting to read with error replacement.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733744956.3984566}, "269f62bebcabbb203d4234cffc6214cff6ea06069a79d5aba4f5b046362b476b": {"value": {"summary": "Manages code chunking operations for various programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a single chunk."}, {"name": "overlap", "type": "int", "description": "The number of tokens to overlap between chunks."}], "returns": {"type": "None", "description": "This constructor initializes the `ChunkManager` with specified token limits and prepares the token manager."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the provided Python code."}, {"exception": "Exception", "description": "Raised for any other errors during chunk creation."}], "description": "The `ChunkManager` class is designed to handle the creation, splitting, and merging of code chunks. It provides methods to create chunks of code with context awareness, specifically for Python using Abstract Syntax Tree (AST) analysis, and for other languages using simple line-based token limits. The class also supports splitting a chunk at a specified line and merging two contiguous chunks. It leverages a `TokenManager` to count tokens and manage chunk sizes effectively.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733745761.8376768}, "afa02be2d0dfa10ad5fe3274f9327dc5ca602692ee4e5601924e5e9975ada9e5": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "self", "type": "DependencyAnalyzer", "description": "An instance of the DependencyAnalyzer class."}, {"name": "code", "type": "str", "description": "The source code to be analyzed for dependencies."}], "returns": {"type": "Set[str]", "description": "A set of strings representing the dependencies found in the provided code."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the provided source code, preventing successful parsing."}], "description": "The DependencyAnalyzer class utilizes Python's Abstract Syntax Tree (AST) to analyze and identify dependencies within a given Python source code. It traverses the AST nodes to detect import statements, imported names, and function calls, thereby determining which external modules and functions the code depends on. This class is particularly useful for static code analysis, helping developers understand module dependencies and manage them effectively.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733745767.8017838}, "4a944ae6fc912b793eff50fa301839809f7eed33e1f57e1e03f1030c87b7e8c5": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read. Can be a string or a Path object."}, {"name": "use_cache", "type": "bool", "description": "Flag to determine if cached content should be used. Defaults to True."}, {"name": "encoding", "type": "str", "description": "The encoding to use when reading the file. Defaults to 'utf-8'."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs during reading."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised if the file cannot be decoded using the specified encoding."}, {"exception": "Exception", "description": "General exception raised for any other errors encountered during file reading."}], "description": "The `FileHandler` class provides methods to perform asynchronous file operations with built-in caching and error handling mechanisms. It is designed to efficiently read file contents while managing a cache to minimize redundant I/O operations. The class also includes error handling for common issues such as encoding errors, ensuring robust file reading capabilities.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733745773.5290074}, "d183c9a244a5d64da40da423908a826a66a080fabef6430ce03b7c7839b64a92": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [], "description": "The MetricsCalculationError class serves as the base exception for errors encountered during metrics calculations. It inherits from the standard Exception class, allowing it to be used in try-except blocks to handle specific errors related to metrics calculations.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733745776.6212935}, "6414920998d1f6f09cd0360a2c08c9a60632aed35bce5d7ec4f42e99954df4e6": {"value": {"summary": "Custom exception for missing code chunks.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is used for exception handling."}, "raises": [], "description": "The `ChunkNotFoundError` class is a custom exception that is raised when a requested code chunk cannot be found within the context management system. This exception helps in identifying scenarios where a specific chunk of code is expected but is not available, allowing for graceful error handling in such cases.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733745780.516675}, "503af7d437f72d01de9d43267b3a46ed5226af5b914e9665db8368df72b65b9c": {"value": {"summary": "A class for interacting with OpenAI's GPT-4O model to generate prompts, calculate tokens, and create documentation.", "args": [{"name": "api_key", "type": "str", "description": "The API key used to authenticate requests to the OpenAI API."}], "returns": {"type": "OpenAIModel", "description": "An instance of the OpenAIModel class."}, "raises": [{"exception": "openai.error.OpenAIError", "description": "Raised when there is an error with the OpenAI API request."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error in encoding the schema to JSON format."}], "description": "The OpenAIModel class provides methods to interact with OpenAI's GPT-4O model. It allows users to generate structured prompts, estimate the number of tokens in a prompt, and generate documentation using the model. The class requires an API key to authenticate requests to the OpenAI API. It uses Python's logging module to log actions performed within the class methods.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733745791.5338497}, "730dfcc92ddfe83fa6f94dab3d9c368164b47013cc3226513a466eb8a487a391": {"value": {"summary": "Base exception for documentation-related errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [], "description": "The `DocumentationError` class serves as the base exception for all errors related to documentation processing. It is a subclass of the built-in `Exception` class and is used to signal issues that occur during the documentation generation process.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733745793.1316469}, "3225f3b57c378266473d506bba9ba0da5db2cda6a7f55191d8e1382f9f8cf934": {"value": {"summary": "Handles interaction with the Gemini model API.", "args": [{"name": "api_key", "type": "str", "description": "The API key used for authenticating requests to the Gemini model API."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL of the Gemini model API."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised if there is an issue with the HTTP request to the Gemini model API."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error decoding the JSON response from the API."}], "description": "The `GeminiModel` class is responsible for managing interactions with the Gemini model API. It provides functionalities to generate documentation, calculate token counts for prompts, and create structured prompts for the Gemini model. The class utilizes asynchronous HTTP requests to communicate with the API and uses a token management utility to handle token calculations.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733745796.072491}, "89e8355fd2f982171607539c38535567b78d8d1af22caf4609e66d325a4814cf": {"value": {"summary": "API request model for documentation generation.", "args": [{"name": "file_paths", "type": "List[str]", "description": "A list of file paths to be processed for documentation generation."}, {"name": "skip_types", "type": "Optional[List[str]]", "description": "A list of file types to be skipped during processing."}, {"name": "project_info", "type": "Optional[str]", "description": "Information about the project for which documentation is being generated."}, {"name": "style_guidelines", "type": "Optional[str]", "description": "Style guidelines to be followed during documentation generation."}, {"name": "safe_mode", "type": "Optional[bool]", "description": "A flag indicating whether to run the process in safe mode."}, {"name": "project_id", "type": "str", "description": "A unique identifier for the project."}, {"name": "provider", "type": "str", "description": "The provider to be used for documentation generation, defaulting to 'azure'."}, {"name": "max_concurrency", "type": "Optional[int]", "description": "The maximum number of concurrent processes allowed, defaulting to 5."}, {"name": "priority", "type": "Optional[str]", "description": "The priority level of the task, which can be 'low', 'normal', or 'high'."}, {"name": "callback_url", "type": "Optional[str]", "description": "A URL to be called back upon task completion."}], "returns": {"type": "DocumentationRequest", "description": "An instance of DocumentationRequest with validated attributes."}, "raises": [{"exception": "ValueError", "description": "Raised if the provider is not one of the valid options ('azure', 'gemini', 'openai')."}, {"exception": "ValueError", "description": "Raised if the priority is not 'low', 'normal', or 'high'."}], "description": "The DocumentationRequest class serves as the model for API requests related to documentation generation. It includes attributes that define the parameters for the documentation process, such as file paths, project information, style guidelines, and provider details. The class also includes validators to ensure that the provider and priority values are valid.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733745807.5635438}, "4c5046e3757e6b00ea13a35cd9d062979b7df8a3c2bfa31be626e0e5619b31d7": {"value": {"summary": "Enumeration of supported tokenizer models.", "args": [], "returns": {"type": "None", "description": "This class does not return a value as it serves as an enumeration."}, "raises": [], "description": "The `TokenizerModel` class is an enumeration that defines the supported tokenizer models for the tokenization operations. It provides a clear and structured way to refer to different models, such as GPT-4, GPT-3, and Codex, by their specific encoding names. This class is used to specify the model type in various tokenization processes within the module.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733745810.1270359}, "4b9eb22e2286cd8dab6d561ddd7ddda77925c3dee8ea3cfe3f1c9aaa573c7cd2": {"value": {"summary": "Manages Jinja2 templates for rendering.", "args": [{"name": "template_dir", "type": "str", "description": "Directory containing the templates."}, {"name": "template_name", "type": "str", "description": "Name of the template to retrieve or render."}, {"name": "context", "type": "dict", "description": "Dictionary of variables to pass to the template for rendering."}, {"name": "name", "type": "str", "description": "Name to reference the custom template."}, {"name": "template_path", "type": "str", "description": "Path to the custom template file."}], "returns": {"type": "Template", "description": "The requested template object."}, "raises": [{"exception": "ValueError", "description": "Raised when an invalid template name is provided."}, {"exception": "FileNotFoundError", "description": "Raised when the specified template file does not exist."}, {"exception": "Exception", "description": "Raised for general errors during template loading or rendering."}], "description": "The TemplateManager class is responsible for managing and rendering Jinja2 templates. It initializes with a specified directory containing template files and uses caching to optimize template retrieval and rendering. The class provides methods to get, render, and add custom templates, handling exceptions and logging errors as needed.", "metadata": {}, "complexity": 9, "validation_status": true, "validation_errors": []}, "timestamp": 1733745816.4959967}, "0bca18d20b43ccb3ad752b47b00cfea5b897b9e7d96c5afcfd539d90457a41fd": {"value": {"summary": "Configuration model and loader for AI providers.", "args": [{"name": "name", "type": "str", "description": "The name of the AI provider."}, {"name": "endpoint", "type": "str", "description": "The API endpoint for the provider."}, {"name": "api_key", "type": "str", "description": "The API key for authenticating with the provider."}, {"name": "deployment_name", "type": "Optional[str]", "description": "The name of the deployment, if applicable."}, {"name": "api_version", "type": "Optional[str]", "description": "The version of the API to use."}, {"name": "model_name", "type": "Optional[str]", "description": "The name of the model to use."}, {"name": "max_tokens", "type": "int", "description": "The maximum number of tokens allowed in a request, default is 4096."}, {"name": "temperature", "type": "float", "description": "The sampling temperature, must be between 0 and 1, default is 0.7."}, {"name": "max_retries", "type": "int", "description": "The maximum number of retries for a request, default is 3."}, {"name": "retry_delay", "type": "float", "description": "The delay between retries, in seconds, default is 1.0."}, {"name": "cache_enabled", "type": "bool", "description": "Flag to enable or disable caching, default is True."}, {"name": "timeout", "type": "float", "description": "The timeout for requests, in seconds, default is 30.0."}, {"name": "chunk_overlap", "type": "int", "description": "The overlap between chunks, default is 200."}, {"name": "min_chunk_size", "type": "int", "description": "The minimum size of a chunk, default is 100."}, {"name": "max_parallel_chunks", "type": "int", "description": "The maximum number of parallel chunks, default is 3."}], "returns": {"type": "Dict[str, ProviderConfig]", "description": "A dictionary of provider configurations."}, "raises": [{"exception": "ValueError", "description": "Raised if the temperature is not between 0 and 1, or if max_tokens is not between 1 and 8192."}, {"exception": "FileNotFoundError", "description": "Raised if the specified configuration file does not exist."}, {"exception": "ValueError", "description": "Raised if the configuration file format is unsupported or if loading the file fails."}], "description": "This module defines a configuration model for AI providers using Pydantic's BaseModel. It includes attributes for various configuration settings such as API keys, endpoints, and model parameters. Additionally, it provides a function to load these configurations from either a file or environment variables. The configuration model ensures that the settings are validated and conform to expected ranges, such as temperature and max_tokens.", "metadata": {}, "complexity": 4, "validation_status": true, "validation_errors": []}, "timestamp": 1733745823.2853398}, "3cfac8630b801dd4066491e462781516ade638e7b732793642463d14978397f8": {"value": {"summary": "Calculates total tokens for the prompt content using TokenManager.", "args": [{"name": "base_info", "type": "str", "description": "Project and style information that provides context for the prompt."}, {"name": "context", "type": "str", "description": "Related code or documentation that adds additional context to the prompt."}, {"name": "chunk_content", "type": "str", "description": "The content of the specific chunk being documented or processed."}, {"name": "schema", "type": "str", "description": "A JSON schema that defines the structure of the data being processed."}], "returns": {"type": "int", "description": "The total count of tokens for the provided prompt content."}, "raises": [{"exception": "TokenManagerError", "description": "Raised if there is an error in counting tokens using the TokenManager."}], "description": "This function calculates the total number of tokens required for a given set of prompt content, which includes base information, context, chunk content, and a JSON schema. It utilizes the TokenManager to count tokens for each piece of text and sums them up to provide the total token count. This is useful in scenarios where token limits are imposed, such as in API requests or data processing tasks.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733745835.7992826}, "31a5d7507863b83eca644270e185e7ee4fc4181f680da04c0c2d31059d385cee": {"value": {"summary": "Handles API interactions with error handling and rate limiting.", "args": [{"name": "session", "type": "aiohttp.ClientSession", "description": "The aiohttp client session used for making HTTP requests."}, {"name": "config", "type": "ProviderConfig", "description": "Configuration settings for the API provider, including API keys and endpoints."}, {"name": "semaphore", "type": "asyncio.Semaphore", "description": "A semaphore to limit the number of concurrent API requests."}, {"name": "metrics_manager", "type": "MetricsManager", "description": "An instance of MetricsManager to record metrics related to API calls."}], "returns": {"type": "None", "description": "This class does not return a value upon instantiation."}, "raises": [{"exception": "Exception", "description": "General exception raised during API interactions, including rate limit and network errors."}], "description": "The `APIHandler` class is responsible for managing interactions with various AI service providers' APIs. It includes mechanisms for handling errors, managing rate limits, and retrying failed requests. This class is designed to work asynchronously and is optimized for efficient API usage by implementing rate limiting and retry logic. It supports multiple providers including Azure, Gemini, and OpenAI, and uses a semaphore to control concurrent API requests.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733745840.5231843}, "e109a717920fc85efdb1ed64919479b14acacddaf6aa50f078a8610a91ea2277": {"value": {"summary": "Main module for generating and inserting docstrings using AI models.", "args": [{"name": "repo_path", "type": "str", "description": "Path to the code repository where docstrings need to be inserted."}, {"name": "config", "type": "str", "description": "Path to the configuration file (default is 'config.json')."}, {"name": "provider", "type": "str", "description": "AI provider to use for generating docstrings. Options are 'azure', 'gemini', or 'openai' (default is 'azure')."}, {"name": "concurrency", "type": "int", "description": "Number of concurrent requests to make (default is 5)."}, {"name": "skip_types", "type": "str", "description": "Comma-separated list of file extensions to skip during processing."}, {"name": "project_info", "type": "str", "description": "Additional information about the project."}, {"name": "style_guidelines", "type": "str", "description": "Documentation style guidelines to follow."}, {"name": "safe_mode", "type": "bool", "description": "If set, runs in safe mode where no files are modified."}, {"name": "log_level", "type": "str", "description": "Logging level to use (default is 'INFO')."}, {"name": "schema", "type": "str", "description": "Path to the function schema file (default is 'schemas/function_schema.json')."}, {"name": "doc_output_dir", "type": "str", "description": "Directory to save the generated documentation files (default is 'documentation')."}, {"name": "project_id", "type": "str", "description": "Unique identifier for the project."}], "returns": {"type": "None", "description": "This function does not return any value. It performs operations and logs results."}, "raises": [{"exception": "SystemExit", "description": "Raised when the program needs to exit due to an error or user interruption."}, {"exception": "Exception", "description": "Raised for any unhandled exceptions during the execution of the main function."}], "description": "This module serves as the main entry point for generating and inserting docstrings into a codebase using AI models from Azure, Gemini, or OpenAI. It parses command-line arguments, sets up logging, loads configurations, and manages the documentation generation process asynchronously. The module supports concurrency and safe mode operations, and it handles various configurations and provider-specific settings.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733745853.787636}, "17b902623869b63dfd6d5029f9d636e195e6c1eea92f22d16948c8c3f62a509e": {"value": {"summary": "Handles interaction with the Azure OpenAI API.", "args": [{"name": "api_key", "type": "str", "description": "The API key for authenticating with the Azure OpenAI service."}, {"name": "endpoint", "type": "str", "description": "The endpoint URL for the Azure OpenAI service."}, {"name": "deployment_name", "type": "str", "description": "The name of the deployment within the Azure OpenAI service."}, {"name": "api_version", "type": "str", "description": "The version of the Azure OpenAI API to use."}], "returns": {"type": "AzureModel", "description": "An instance of the AzureModel class configured with the provided API details."}, "raises": [{"exception": "aiohttp.ClientError", "description": "Raised if there is an issue with the HTTP request."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error decoding the JSON response from the API."}, {"exception": "Exception", "description": "General exception for any other errors that may occur."}], "description": "The `AzureModel` class provides methods to interact with the Azure OpenAI API, including generating documentation, calculating token counts, and creating prompts. It manages the API requests and handles the responses, ensuring seamless communication with the Azure OpenAI services. This class is designed to facilitate the generation of documentation by leveraging the capabilities of the Azure OpenAI API, making it easier to automate documentation tasks.", "metadata": {}, "complexity": 3, "validation_status": true, "validation_errors": []}, "timestamp": 1733745859.102628}, "63bb1ff369ef22e9c078898606b2e9bb99527cb93f3bb84cd8e597f4b9a10d06": {"value": {"summary": "Handles Python code analysis and transformation.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "Schema defining the structure of functions to be validated against."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "Analyzer for calculating and storing code metrics."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the provided code."}, {"exception": "ValidationError", "description": "Raised when the code structure does not conform to the provided schema."}, {"exception": "Exception", "description": "Raised for any other errors that occur during processing."}], "description": "The PythonHandler class provides methods for analyzing Python code structure, inserting docstrings, and validating code using pylint. It leverages the AST module for parsing and manipulating code, and integrates with metrics and validation tools to ensure code quality and adherence to schemas.", "metadata": {}, "complexity": 30, "validation_status": true, "validation_errors": []}, "timestamp": 1733745861.4718897}, "a1ce3dd2c84fc55a3868dde2edc06db7445976e3d9d43efe28a4f54d9e342c89": {"value": {"summary": "Handler for the CSS programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [], "description": "The `CSSHandler` class is designed to manage CSS code files by providing functionalities to extract code structure, insert documentation comments, and validate the CSS code. It extends the `BaseHandler` abstract class and utilizes external JavaScript scripts for parsing and modifying CSS code. The class is particularly useful for developers who need to automate the process of documenting and validating CSS files.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733745867.2106562}, "1edfe9834d8f10db1c1b1bd8d58b58a17c9f741dce51a569287e7a94414d2904": {"value": {"summary": "Handler for HTML language operations.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations, providing necessary configurations and settings for handling HTML code."}], "returns": {"type": "HTMLHandler", "description": "An instance of the HTMLHandler class initialized with the provided function schema."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised when there is an error executing the external JavaScript parser or inserter scripts."}, {"exception": "json.JSONDecodeError", "description": "Raised when there is an error decoding JSON output from the parser script."}, {"exception": "FileNotFoundError", "description": "Raised when the 'tidy' HTML validator is not installed or not found in the system PATH."}, {"exception": "Exception", "description": "Raised for any unexpected errors during HTML structure extraction, comment insertion, or code validation."}], "description": "The `HTMLHandler` class is designed to manage operations related to HTML code, such as extracting structure, inserting comments, and validating code. It extends the `BaseHandler` class and utilizes external JavaScript scripts for parsing and modifying HTML code. This handler is particularly useful for developers working with HTML files, providing automated tools to enhance code quality and documentation.", "metadata": {}, "complexity": 12, "validation_status": true, "validation_errors": []}, "timestamp": 1733745870.8517528}, "b647f2662bd60f31bab9bcbd6b7128651efa29a1cdc9fdb428b924d5d12ebf61": {"value": {"summary": "Handler for the Go programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "None", "description": "This class does not return a value upon initialization."}, "raises": [], "description": "The `GoHandler` class is designed to manage Go programming language code files. It extends the `BaseHandler` abstract class and provides methods to extract the structure of Go code, insert documentation comments, and validate the code for syntax and potential issues. The class leverages external Go scripts to parse and modify code, ensuring that the code structure is accurately represented and documented. It also uses Go tools like 'go fmt' and 'go vet' for code validation.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733745877.620263}, "b09f919c4cf3e62726ada9b486d81504ecf267a3244dcccb6193a019f1b5fde5": {"value": {"summary": "Handler for Java language operations.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema used for function operations."}], "returns": {"type": "JavaHandler", "description": "An instance of JavaHandler initialized with the provided function schema."}, "raises": [], "description": "The `JavaHandler` class is designed to handle various operations related to Java code, such as extracting code structure, inserting Javadoc comments, and validating the code. It extends the `BaseHandler` abstract class and utilizes external JavaScript scripts for parsing and modifying Java code. The class is initialized with a function schema that guides its operations.", "metadata": {}, "complexity": 14, "validation_status": true, "validation_errors": []}, "timestamp": 1733745879.8570325}, "476ae94cbf6b9a8b28c174f90be325feadafd47374414f89d3655d8606ec303a": {"value": {"summary": "Handler for the C++ programming language.", "args": [{"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions for documentation generation."}], "returns": {"type": "None", "description": "This constructor does not return a value."}, "raises": [{"exception": "subprocess.CalledProcessError", "description": "Raised if there is an error during the execution of the C++ parser or inserter executables."}, {"exception": "json.JSONDecodeError", "description": "Raised if there is an error parsing the output from the C++ parser."}, {"exception": "FileNotFoundError", "description": "Raised if the 'g++' compiler is not installed or not found in the PATH."}, {"exception": "Exception", "description": "Raised for any unexpected errors during code extraction, insertion, or validation."}], "description": "The `CppHandler` class is designed to handle C++ code files, providing functionalities such as extracting code structure, inserting documentation comments, and validating code syntax. It extends the `BaseHandler` abstract class and utilizes external C++ scripts for parsing and modifying code. The class is initialized with a function schema that defines functions for documentation generation.", "metadata": {}, "complexity": 17, "validation_status": true, "validation_errors": []}, "timestamp": 1733745884.0578148}, "42bc622e6d3dbbce039f2067b63e62eda3da178689227e0cc404cbeeb0458fe6": {"value": {"summary": "Enumeration for JavaScript and TypeScript documentation styles.", "args": [], "returns": {"type": "JSDocStyle", "description": "An instance of the JSDocStyle enumeration representing a documentation style."}, "raises": [], "description": "The `JSDocStyle` class is an enumeration that defines two styles of documentation comments used in JavaScript and TypeScript codebases: JSDoc and TSDoc. These styles are used to annotate code with comments that can be processed by documentation generators.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733745886.7721841}, "1365e42e11a7d571f54456db20472204b671efcdbf970fbf2ced0db308558bb6": {"value": {"summary": "Utility functions for handling programming languages in documentation generation.", "args": [{"name": "language", "type": "str", "description": "The programming language of the source code (e.g., 'python', 'java', 'javascript')."}, {"name": "function_schema", "type": "Dict[str, Any]", "description": "The schema defining functions and their documentation structure."}], "returns": {"type": "Optional[BaseHandler]", "description": "An instance of the corresponding language handler or None if the language is unsupported."}, "raises": [{"exception": "None", "description": "This function does not raise any exceptions."}], "description": "The `language_functions` module provides essential utility functions to manage different programming languages within a documentation generation pipeline. It includes functions to determine the appropriate language handler for a given programming language and to insert AI-generated docstrings or comments into source code. The module supports various languages such as Python, Java, JavaScript, TypeScript, Go, C++, HTML, and CSS. It dynamically loads function schemas and uses specific handlers to integrate documentation seamlessly into the code.", "metadata": {}, "complexity": 5, "validation_status": true, "validation_errors": []}, "timestamp": 1733745896.040289}, "e746411d3bf00294563b497ed3729070134175ba51129ffe9e81d670836e16eb": {"value": {"summary": "Factory function to retrieve language-specific code handlers.", "args": [{"name": "language", "type": "str", "description": "The programming language of the source code, e.g., 'python', 'java', 'javascript', etc."}, {"name": "function_schema", "type": "Dict[str, Any]", "description": "A dictionary representing the schema that defines the functions within the source code."}, {"name": "metrics_analyzer", "type": "MetricsAnalyzer", "description": "An instance of the MetricsAnalyzer class used to analyze code metrics and performance."}], "returns": {"type": "Optional[BaseHandler]", "description": "An instance of the corresponding language handler if supported; otherwise, None."}, "raises": [{"exception": "None", "description": "The function does not raise exceptions. It returns None if the language is unsupported or the function schema is None."}], "description": "This function serves as a factory method to obtain the appropriate handler for a given programming language. It utilizes a mapping of supported languages to their respective handler classes, which are responsible for processing code structures, inserting documentation comments, and validating code. The function requires a language identifier, a function schema, and a metrics analyzer to instantiate the correct handler. If the language is unsupported or the function schema is None, the function returns None.", "metadata": {}, "complexity": 4, "validation_status": true, "validation_errors": []}, "timestamp": 1733745901.8485487}, "bce9a560784acda5fa19868a6d9ccad9c9b5609a4be76fb629a78ab71a9b4192": {"value": {"summary": "Manages code chunking operations for various programming languages.", "args": [{"name": "max_tokens", "type": "int", "description": "Maximum number of tokens allowed in a single chunk. Default is 4096."}, {"name": "overlap", "type": "int", "description": "Number of tokens allowed to overlap between consecutive chunks. Default is 200."}], "returns": {"type": "ChunkManager", "description": "An instance of the ChunkManager class initialized with specified max_tokens and overlap."}, "raises": [{"exception": "SyntaxError", "description": "Raised when there is a syntax error in the Python code being chunked."}, {"exception": "Exception", "description": "Raised for any general error that occurs during the chunking process."}, {"exception": "ValueError", "description": "Raised when invalid parameters are provided for splitting or merging chunks."}], "description": "The ChunkManager class provides functionality to create, split, and merge code chunks. It uses an abstract syntax tree (AST) for Python code to ensure context-aware chunking and employs a simple line-based approach for other languages. The class is designed to handle large codebases by breaking them into manageable chunks based on a specified maximum token limit and overlap. It also includes methods to split and merge existing chunks, ensuring flexibility in managing code segments.", "metadata": {}, "complexity": 29, "validation_status": true, "validation_errors": []}, "timestamp": 1733747830.9251783}, "1be540eeb4e56349a5e8304eff6349817a455897338ce18576930cc75847e347": {"value": {"summary": "Analyzes dependencies within a Python file using AST.", "args": [{"name": "code", "type": "str", "description": "The source code to analyze for dependencies."}, {"name": "node", "type": "ast.Import", "description": "The import node to visit and record imported module names."}, {"name": "node", "type": "ast.ImportFrom", "description": "The import-from node to visit and record imported module and object names."}, {"name": "node", "type": "ast.Name", "description": "The name node to visit and add to dependencies if used and imported."}, {"name": "node", "type": "ast.Call", "description": "The call node to visit and add function names to dependencies if imported."}], "returns": {"type": "Set[str]", "description": "A set of dependencies found in the code."}, "raises": [{"exception": "SyntaxError", "description": "Raised if there is a syntax error in the provided source code during parsing."}], "description": "The `DependencyAnalyzer` class is designed to analyze Python source code to identify dependencies by traversing the Abstract Syntax Tree (AST). It extends the `ast.NodeVisitor` class to visit different types of nodes in the AST, such as import statements, name nodes, and call nodes. The class collects and returns a set of dependencies, which are the modules and objects imported and used within the code. This can be useful for understanding code dependencies, refactoring, or preparing for code migration.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733747842.3914702}, "5174b217d66fd1302f774696ca9cf1b823cbc46c3485faa04b007c0ac513caf7": {"value": {"summary": "Handles file operations with caching and error handling.", "args": [{"name": "file_path", "type": "Union[str, Path]", "description": "The path to the file to be read."}, {"name": "use_cache", "type": "bool", "description": "Flag indicating whether to use the cache for reading the file."}, {"name": "encoding", "type": "str", "description": "The encoding to use for reading the file."}], "returns": {"type": "Optional[str]", "description": "The content of the file as a string, or None if an error occurs."}, "raises": [{"exception": "UnicodeDecodeError", "description": "Raised if there is an encoding issue with the file."}, {"exception": "Exception", "description": "Raised for other general exceptions that may occur during file reading."}], "description": "The FileHandler class provides asynchronous file reading capabilities with support for caching and error handling. It is designed to efficiently read file content using asynchronous I/O operations, while maintaining a cache for frequently accessed files to improve performance. The class also handles common file reading errors, such as Unicode decoding issues, by attempting to read files with error handling mechanisms.", "metadata": {}, "complexity": 10, "validation_status": true, "validation_errors": []}, "timestamp": 1733747845.4457362}, "6b79ac8c68ecb516abac131f6dd74567f9d1e8aba1cfe8ab41fba2c87dd64270": {"value": {"summary": "Base exception for metrics calculation errors.", "args": [], "returns": {"type": "None", "description": "This class does not return any value as it is an exception class."}, "raises": [], "description": "The MetricsCalculationError class serves as the base exception for errors encountered during the calculation of metrics. It extends the built-in Exception class and does not add any additional functionality or attributes. This exception is intended to be used as a parent class for more specific metric-related exceptions, providing a clear and consistent exception hierarchy for metric calculation errors.", "metadata": {}, "complexity": 1, "validation_status": true, "validation_errors": []}, "timestamp": 1733747847.4328794}}